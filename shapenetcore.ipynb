{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6399893,"sourceType":"datasetVersion","datasetId":3689970}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:17:34.607357Z","iopub.execute_input":"2024-04-23T04:17:34.607748Z","iopub.status.idle":"2024-04-23T04:17:38.100905Z","shell.execute_reply.started":"2024-04-23T04:17:34.607718Z","shell.execute_reply":"2024-04-23T04:17:38.099959Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def knn(data, k=5)->torch.Tensor:\n    \"\"\"Construct edge feature for each point\n    Args:\n      point_cloud: (batch_size, num_points, num_dims)\n      k: int number of neighbours\n\n    Returns:\n      idx: shape:(batch_size, num_points, nums_neighours,)\n    \"\"\"\n    dists_matrix = torch.cdist(data, data)\n    #print(dists_matrix.shape)\n    _, idx = dists_matrix.topk(k+1, dim=-1, largest=False)  # +1 the point itself is included\n    return idx[...,1:] # not include the point itself\n\n\n\ndef get_edge_feature(point_cloud, idx=None, k=20,device=\"cpu\"):\n    \"\"\"Construct edge feature for each point\n    Args:\n      point_cloud: (batch_size, num_points, num_dims)\n      idx: (batch_size, num_points, neighbours)\n      k: int\n      device: cpu/cuda\n\n    Returns:\n      features: (batch_size, num_dims ,num_points, k)\n    \"\"\"\n    point_cloud = point_cloud.to(device)\n    batch_size = point_cloud.shape[0]\n    num_points = point_cloud.shape[1]\n\n    if(idx==None):\n        idx = knn(point_cloud,k=k) # (batch_size, num_points, nums_neighours,)\n\n    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1) * num_points # create the base index for mapping\n    idx = idx.to(device=device)\n    idx = idx + idx_base #[0...0...0]->[0...100...200]\n    idx=idx.view(-1) # flatten it -> tensor([  0,  56,  25,  ..., 225, 222, 271], device='cuda:0') e.g: [K01,K02,K03,K11,K12,K13...] shape = (B*N*K) \n   \n    num_dims = point_cloud.shape[2]\n\n    # feature : turn neighbour index in idx to coordinate\n    feature = point_cloud.view(batch_size*num_points, -1)[idx, :] # feature : B*N*F -> BN * F -> (B*N*K) * F\n    # feature : reshape into (Batch_size * Num_points *Nums_neigbours * Features)\n    feature = feature.view(batch_size, num_points, k, num_dims)\n    \n    # pointcloud : create replicate of the self point up to k for matching feature - size B*N*K(repeated)*F \n    point_cloud = point_cloud.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1) \n\n    # feature size B*N*K*F -> B*N*K*2F (feature-x || x)\n    feature = torch.cat((feature-point_cloud, point_cloud), dim=3)\n\n    # (B * 2F * N * K) for later conv each coordinate(F)\n    feature=feature.permute(0,3,1,2).contiguous()\n\n    return feature\n\n# Example usage:\ndata = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\n#neighbors = knn(data, k=4)\nedges= get_edge_feature(data)\nprint(edges.shape)\nprint(type(edges))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:17:38.102766Z","iopub.execute_input":"2024-04-23T04:17:38.103164Z","iopub.status.idle":"2024-04-23T04:17:38.242455Z","shell.execute_reply.started":"2024-04-23T04:17:38.103122Z","shell.execute_reply":"2024-04-23T04:17:38.241497Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"torch.Size([3, 6, 100, 20])\n<class 'torch.Tensor'>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Edgeconv**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EdgeConv(nn.Module):\n    def __init__(self, in_channels, out_channels, num_neighbours=20,device=\"cpu\"):\n        \"\"\"Setup EdgeConv\n        Args:\n        in_channels: int\n        out_channels: int\n        num_neighbours: int\n        \"\"\"\n        super(EdgeConv, self).__init__()\n        self.device=device\n        self.k= num_neighbours\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels*2, out_channels=out_channels, kernel_size=1, bias=False,device=self.device),\n            nn.BatchNorm2d(out_channels,device=self.device),\n            nn.LeakyReLU(negative_slope=0.2)\n        )\n\n    def forward(self,x):\n        \"\"\"Setup EdgeConv\n        Args:\n        x: shape - (batch_size, num_points, num_dims)\n\n        Returns:\n        features: (batch_size, num_dims, num_points, num_neigbours)\n        \"\"\"\n        x = get_edge_feature(x, k=self.k,device=self.device) #(batch_size, num_points, dim) -> (batch_size, dim*2, num_points ,k)\n        x = self.conv(x)\n        # for each point pick the largest k (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n        x = x.max(dim=-1, keepdim=False)[0]\n        x = x.permute(0,2,1).contiguous()\n        return x\n    \n# Example usage:\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndata = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\nconv = EdgeConv(3, 64,device=device)\nout = conv(data)\nprint(\"out.shape=\", out.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:17:38.243871Z","iopub.execute_input":"2024-04-23T04:17:38.244246Z","iopub.status.idle":"2024-04-23T04:17:39.254371Z","shell.execute_reply.started":"2024-04-23T04:17:38.244212Z","shell.execute_reply":"2024-04-23T04:17:39.253468Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"out.shape= torch.Size([3, 100, 64])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **DGCNN (Classification)**","metadata":{}},{"cell_type":"code","source":"class DGCNN(nn.Module):\n    def __init__(self, num_neighbours=20,out_channels=40,dropout_rate =0.3,device=\"cpu\"):\n        super(DGCNN,self).__init__()\n        self.inChannels=[3,64,64,128,256]\n        self.edgeConv0 = EdgeConv(in_channels=3,out_channels=64,num_neighbours=num_neighbours,device=device)\n        self.edgeConv1 = EdgeConv(in_channels=64,out_channels=64,num_neighbours=num_neighbours,device=device)\n        self.edgeConv2 = EdgeConv(in_channels=64,out_channels=128,num_neighbours=num_neighbours,device=device)\n        self.edgeConv3 = EdgeConv(in_channels=128,out_channels=256,num_neighbours=num_neighbours,device=device)\n\n        self.edgeConv4 = EdgeConv(in_channels=512,out_channels=1024,num_neighbours=num_neighbours,device=device)\n\n        self.linear1 = nn.Linear(2048, 512, bias=False,device=device)\n        self.bn1 = nn.BatchNorm1d(512,device=device)\n        self.drop1 = nn.Dropout(dropout_rate)\n        self.linear2 = nn.Linear(512, 256, bias=False,device=device)\n        self.bn2 = nn.BatchNorm1d(256,device=device)\n        self.drop2 = nn.Dropout(dropout_rate)\n        self.linear3 = nn.Linear(256,out_channels, bias=False,device=device)\n\n\n    def forward(self,x):\n        x0=self.edgeConv0(x)\n        #print(\"x0:\",x0.shape)\n        x1=self.edgeConv1(x0)\n        x2=self.edgeConv2(x1)\n        x3=self.edgeConv3(x2)\n\n        x=torch.cat((x0,x1,x2,x3),dim=2)\n        \n        x= self.edgeConv4(x) # (batch_size, num_points ,64+64+128+256) -> (batch_size, num_points, emb_dims(1024))\n        \n        #todo \n        # maxpool and avgpool\n        x= x.permute(0,2,1).contiguous()\n        maxPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n        avgPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n        x=torch.cat((maxPoolX,avgPoolX),dim = 1) #(batch_size, 2048)\n        \n        #mlp[512,256,c(40)]\n        x= F.leaky_relu(self.bn1(self.linear1(x)))\n        x=self.drop1(x)\n        x= F.leaky_relu(self.bn2(self.linear2(x)))\n        x=self.drop2(x)\n        x= self.linear3(x)\n        # output\n\n        return x\n    \n# Example usage:\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndata = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\nprint(\"data shape: \", data.shape)\nprint(device)\ndgcnn = DGCNN(device=device)\nout = dgcnn(data)\n\nprint(\"out.shape=\", out.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:17:39.257050Z","iopub.execute_input":"2024-04-23T04:17:39.257665Z","iopub.status.idle":"2024-04-23T04:17:39.382226Z","shell.execute_reply.started":"2024-04-23T04:17:39.257630Z","shell.execute_reply":"2024-04-23T04:17:39.381250Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"data shape:  torch.Size([3, 100, 3])\ncuda\nout.shape= torch.Size([3, 40])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset Part","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport json\n#https://www.kaggle.com/datasets/jeremy26/shapenet-core-seg/code \nclass ShapeNet(torch.utils.data.Dataset):\n    def __init__(self, root_dir, split_type, num_samples=2500):\n        self.root_dir = root_dir\n        self.split_type = split_type\n        self.num_samples = num_samples\n        with open(os.path.join(root_dir, f'{self.split_type}_split.json'), 'r') as f:\n            self.split_data = json.load(f)       \n            \n    def __getitem__(self, index):\n        # read point cloud data\n        class_id, class_name, point_cloud_path, seg_label_path = self.split_data[index]\n        \n        # point cloud data\n        point_cloud_path = os.path.join(self.root_dir, point_cloud_path)\n        pc_data = np.load(point_cloud_path)\n        \n        # segmentation labels\n        # -1 is to change part values from [1-16] to [0-15]\n        # which helps when running segmentation\n        pc_seg_labels = np.loadtxt(os.path.join(self.root_dir, seg_label_path)).astype(np.int8) - 1\n#         pc_seg_labels = pc_seg_labels.reshape(pc_seg_labels.size,1)\n        \n        # Sample fixed number of points\n        num_points = pc_data.shape[0]\n        if num_points < self.num_samples:\n            # Duplicate random points if the number of points is less than max_num_points\n            additional_indices = np.random.choice(num_points, self.num_samples - num_points, replace=True)\n            pc_data = np.concatenate((pc_data, pc_data[additional_indices]), axis=0)\n            pc_seg_labels = np.concatenate((pc_seg_labels, pc_seg_labels[additional_indices]), axis=0)\n                \n        else:\n            # Randomly sample max_num_points from the available points\n            random_indices = np.random.choice(num_points, self.num_samples)\n            pc_data = pc_data[random_indices]\n            pc_seg_labels = pc_seg_labels[random_indices]\n        \n        # return variable\n        class_id = torch.tensor([class_id])\n        data_dict= {}\n        data_dict['class_id'] = class_id\n        data_dict['class_name'] = class_name        \n        data_dict['points'] = pc_data \n        data_dict['seg_labels'] = pc_seg_labels \n        return pc_data,class_id        \n                    \n    def __len__(self):\n        return len(self.split_data)        \n        \n_root='/kaggle/input/shapenet-core-seg/Shapenetcore_benchmark/'\ndata = ShapeNet(num_samples=1024,root_dir=_root,split_type='train')\n_data, label= data[0]\nprint(_data.shape)\nprint(label.shape)\nprint(data.__len__())","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:17:39.383640Z","iopub.execute_input":"2024-04-23T04:17:39.384000Z","iopub.status.idle":"2024-04-23T04:17:39.466923Z","shell.execute_reply.started":"2024-04-23T04:17:39.383973Z","shell.execute_reply":"2024-04-23T04:17:39.466033Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(1024, 3)\ntorch.Size([1])\n12137\n","output_type":"stream"}]},{"cell_type":"code","source":"data = ShapeNet(num_samples=1024,root_dir=_root,split_type='test')\nprint(data.__len__())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:17:39.468147Z","iopub.execute_input":"2024-04-23T04:17:39.468501Z","iopub.status.idle":"2024-04-23T04:17:39.487710Z","shell.execute_reply.started":"2024-04-23T04:17:39.468469Z","shell.execute_reply":"2024-04-23T04:17:39.486794Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2848\n","output_type":"stream"}]},{"cell_type":"code","source":"# import os\n# import h5py\n# import torch\n# from torch.utils.data import Dataset\n\n# train_list = ['ply_data_train0.h5', 'ply_data_train1.h5', 'ply_data_train2.h5', 'ply_data_train3.h5', 'ply_data_train4.h5', 'ply_data_train5.h5']\n# test_list = ['ply_data_test0.h5', 'ply_data_test1.h5']\n# val_list = ['ply_data_val0.h5']\n\n# def make_data(mode='train', path='/kaggle/input/model4/shapenet_part_seg_hdf5_data/hdf5_data', num_point=1024):\n#     datas = []\n#     labels = []\n#     if mode == 'train':\n#         for file_list in train_list:\n#             f = h5py.File(os.path.join(path, file_list), 'r')\n#             datas.extend(f['data'][:, :num_point, :])\n#             labels.extend(f['label'])\n#             f.close()\n#     elif mode == 'test':\n#         for file_list in test_list:\n#             f = h5py.File(os.path.join(path, file_list), 'r')\n#             datas.extend(f['data'][:, :num_point, :])\n#             labels.extend(f['label'])\n#             f.close()\n#     else:\n#         for file_list in val_list:\n#             f = h5py.File(os.path.join(path, file_list), 'r')\n#             datas.extend(f['data'][:, :num_point, :])\n#             labels.extend(f['label'])\n#             f.close()\n    \n#     return datas, labels\n\n# class PointDataset(torch.utils.data.Dataset):\n#     def __init__(self, datas, labels):\n#         super().__init__()\n#         self.datas = datas\n#         self.labels = labels\n    \n#     def __getitem__(self, index):\n#         data = torch.tensor(self.datas[index].T.astype('float32'))\n#         label = torch.tensor(self.labels[index].astype('int64'))\n#         data = data.transpose(0, 1)\n#         return data, label\n\n#     def __len__(self):\n#         return len(self.datas)\n\n# datas, labels = make_data(mode='train', num_point=1024)\n# train_dataset = PointDataset(datas, labels)\n# _data, label= train_dataset[0]\n# print(_data.shape)\n# print(label.shape)\n# datas, labels = make_data(mode='val', num_point=1024)\n# val_dataset = PointDataset(datas, labels)\n# _data, label= val_dataset[0]\n# print(_data.shape)\n# print(label.shape)\n# datas, labels = make_data(mode='test', num_point=1024)\n# test_dataset = PointDataset(datas, labels)\n# _data, label= test_dataset[0]\n# print(_data.shape)\n# print(label.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:17:39.488944Z","iopub.execute_input":"2024-04-23T04:17:39.489252Z","iopub.status.idle":"2024-04-23T04:17:39.495228Z","shell.execute_reply.started":"2024-04-23T04:17:39.489227Z","shell.execute_reply":"2024-04-23T04:17:39.494318Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Training part","metadata":{}},{"cell_type":"code","source":"#todo , not finish \nfrom torch.utils.data import DataLoader\nimport os\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport sklearn.metrics as metrics\n\ndef cal_loss(pred, gold, smoothing=True):\n    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n\n    gold = gold.contiguous().view(-1)\n\n    if smoothing:\n        eps = 0.2\n        n_class = pred.size(1)\n\n        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(pred, dim=1)\n\n        loss = -(one_hot * log_prb).sum(dim=1).mean()\n    else:\n        loss = F.cross_entropy(pred, gold, reduction='mean')\n\n    return loss\ndef train(model, epochs, _lr, numPoints,batchSize,device):\n    \"\"\"train\n        Args:\n        model: classifier\n        dataset: shape(batch_size,nums_point,dimemsion)\n        optimizerSelect:1 for SGD, 0 for Adam\n        epochs:train epochs\n        device:\n    \"\"\"\n\n    train_loader = DataLoader(ShapeNet(num_samples=1024,root_dir=_root,split_type='train'),\n                              batch_size=batchSize, shuffle=True)\n\n    test_loader = DataLoader(ShapeNet(num_samples=1024,root_dir=_root,split_type='test'), \n                             batch_size=batchSize, shuffle=True)\n\n\n    #Try to load models\n    model = model\n    opt = optim.Adam(model.parameters(), lr=_lr, weight_decay=1e-4)\n    scheduler = CosineAnnealingLR(opt, epochs, eta_min=_lr)\n    criterion = cal_loss\n    best_test_acc = 0\n    i=0\n    for epoch in range(epochs):\n\n        \n        ####################\n        # Train\n        ####################\n        train_loss = 0.0\n        count = 0.0\n        model.train()\n        train_pred = []\n        train_true = []\n        for data, label in train_loader:\n            data, label = data.to(device), label.to(device).squeeze()\n            batch_size = data.size()[0]\n            opt.zero_grad()\n            logits = model(data)\n            loss = criterion(logits, label)\n            loss.backward()\n            opt.step()\n            preds = logits.max(dim=1)[1]\n            count += batch_size\n            train_loss += loss.item() * batch_size\n            train_true.append(label.cpu().numpy())\n            train_pred.append(preds.detach().cpu().numpy())\n\n\n        train_true = np.concatenate(train_true)\n        train_pred = np.concatenate(train_pred)\n        scheduler.step()\n        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,train_loss*1.0/count,metrics.accuracy_score\n                                                    (train_true, train_pred),metrics.balanced_accuracy_score\n                                                    (train_true, train_pred))\n    \n        print(outstr)\n\n        ####################\n        # Test\n        ####################\n        test_loss = 0.0\n        count = 0.0\n        model.eval()\n        test_pred = []\n        test_true = []\n        for data, label in test_loader:\n            data, label = data.to(device), label.to(device).squeeze()\n            batch_size = data.size()[0]\n            logits = model(data)\n            loss = criterion(logits, label)\n            preds = logits.max(dim=1)[1]\n            count += batch_size\n            test_loss += loss.item() * batch_size\n            test_true.append(label.cpu().numpy())\n            test_pred.append(preds.detach().cpu().numpy())\n\n        test_true = np.concatenate(test_true)\n        test_pred = np.concatenate(test_pred)\n        test_acc = metrics.accuracy_score(test_true, test_pred)\n        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,test_loss*1.0/count,test_acc,avg_per_class_acc)\n        print(outstr)\n        if test_acc >= best_test_acc:\n            best_test_acc = test_acc\n            BASE_DIR = os.getcwd()\n            DATA_DIR = os.path.join(BASE_DIR, 'checkpoints')\n            if not os.path.exists(DATA_DIR):\n                os.mkdir(DATA_DIR)\n                os.mkdir(os.path.join(DATA_DIR, 'models'))\n            torch.save(model.state_dict(), '/kaggle/working/model_shape_5.t7')    \n    \n    \n    return 0\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel=DGCNN(device=device,out_channels=16).to(device)\ntrain(model,5,0.001,1024,16,device)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:17:39.496701Z","iopub.execute_input":"2024-04-23T04:17:39.497290Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Train 0, loss: 1.296550, train acc: 0.904260, train avg acc: 0.592421\nTest 0, loss: 1.169593, test acc: 0.961025, test avg acc: 0.771130\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Run Code","metadata":{}},{"cell_type":"code","source":"## Excute DGCNN\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}