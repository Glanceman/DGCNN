{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":8020542,"datasetId":4726107,"databundleVersionId":8132950},{"sourceType":"datasetVersion","sourceId":8182316,"datasetId":4743220,"databundleVersionId":8305165},{"sourceType":"datasetVersion","sourceId":8166695,"datasetId":4743832,"databundleVersionId":8288771},{"sourceType":"kernelVersion","sourceId":170122193}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## GetEdgeFeature","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef knn(data, k=20)->torch.Tensor:\n    \"\"\"Construct edge feature for each point\n    Args:\n      point_cloud: (batch_size, num_points, num_dims)\n      k: int number of neighbours\n\n    Returns:\n      idx: shape:(batch_size, num_points, nums_neighours,)\n    \"\"\"\n    dists_matrix = torch.cdist(data, data)\n    #print(dists_matrix.shape)\n    _, idx = dists_matrix.topk(k+1, dim=-1, largest=False)  # +1 the point itself is included\n    return idx[...,1:] # not include the point itself\n\ndef get_edge_feature(point_cloud, idx=None, k=20,device=\"cpu\"):\n    \"\"\"Construct edge feature for each point\n    Args:\n      point_cloud: (batch_size, num_points, num_dims)\n      idx: (batch_size, num_points, neighbours)\n      k: int\n      device: cpu/cuda\n\n    Returns:\n      features: (batch_size, num_dims ,num_points, k)\n    \"\"\"\n    point_cloud = point_cloud.to(device)\n    batch_size = point_cloud.shape[0]\n    num_points = point_cloud.shape[1]\n\n    if(idx==None):\n        idx = knn(point_cloud,k=k) # (batch_size, num_points, nums_neighours,)\n\n    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1) * num_points # create the base index for mapping\n    idx = idx.to(device=device)\n    idx = idx + idx_base #[0...0...0]->[0...100...200]\n    idx=idx.view(-1) # flatten it -> tensor([  0,  56,  25,  ..., 225, 222, 271], device='cuda:0') e.g: [K01,K02,K03,K11,K12,K13...] shape = (B*N*K) \n   \n    num_dims = point_cloud.shape[2]\n\n    # feature : turn neighbour index in idx to coordinate\n    feature = point_cloud.view(batch_size*num_points, -1)[idx, :] # feature : B*N*F -> BN * F -> (B*N*K) * F\n    # feature : reshape into (Batch_size * Num_points *Nums_neigbours * Features)\n    feature = feature.view(batch_size, num_points, k, num_dims)\n    \n    # pointcloud : create replicate of the self point up to k for matching feature - size B*N*K(repeated)*F \n    point_cloud = point_cloud.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1) \n\n    # feature size B*N*K*F -> B*N*K*2F (feature-x || x)\n    feature = torch.cat((feature-point_cloud, point_cloud), dim=3)\n\n    # (B * 2F * N * K) for later conv each coordinate(F)\n    feature=feature.permute(0,3,1,2).contiguous()\n\n    return feature","metadata":{"execution":{"iopub.status.busy":"2024-04-22T04:17:07.530717Z","iopub.execute_input":"2024-04-22T04:17:07.531089Z","iopub.status.idle":"2024-04-22T04:17:11.456214Z","shell.execute_reply.started":"2024-04-22T04:17:07.531061Z","shell.execute_reply":"2024-04-22T04:17:11.455332Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## EdgeConv","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EdgeConv(nn.Module):\n    def __init__(self, in_channels, out_channels, num_neighbours=20,device=\"cpu\"):\n        \"\"\"Setup EdgeConv\n        Args:\n        in_channels: int\n        out_channels: int\n        num_neighbours: int\n        \"\"\"\n        super(EdgeConv, self).__init__()\n        self.device=device\n        self.k= num_neighbours\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels*2, out_channels=out_channels, kernel_size=1, bias=False,device=self.device),\n            nn.BatchNorm2d(out_channels,device=self.device),\n            nn.LeakyReLU(negative_slope=0.2)\n        )\n\n    def forward(self,x):\n        \"\"\"Setup EdgeConv\n        Args:\n        x: shape - (batch_size, num_points, num_dims)\n\n        Returns:\n        features: (batch_size, num_dims, num_points, num_neigbours)\n        \"\"\"\n        x = get_edge_feature(x, k=self.k,device=self.device) #(batch_size, num_points, dim) -> (batch_size, dim*2, num_points ,k)\n        x = self.conv(x)\n        # for each point pick the largest k (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n        x = x.max(dim=-1, keepdim=False)[0]\n        x = x.permute(0,2,1).contiguous()\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-22T04:17:20.479926Z","iopub.execute_input":"2024-04-22T04:17:20.480401Z","iopub.status.idle":"2024-04-22T04:17:20.489712Z","shell.execute_reply.started":"2024-04-22T04:17:20.480372Z","shell.execute_reply":"2024-04-22T04:17:20.488663Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## DGCNN - Classification - Teacher","metadata":{}},{"cell_type":"code","source":"class DGCNN(nn.Module):\n    def __init__(self, num_neighbours=20,out_channels=40,dropout_rate =0.3,device=\"cpu\"):\n        super(DGCNN,self).__init__()\n        self.inChannels=[3,64,64,128,256]\n        self.edgeConv0 = EdgeConv(in_channels=3,out_channels=64,num_neighbours=num_neighbours,device=device)\n        self.edgeConv1 = EdgeConv(in_channels=64,out_channels=64,num_neighbours=num_neighbours,device=device)\n        self.edgeConv2 = EdgeConv(in_channels=64,out_channels=128,num_neighbours=num_neighbours,device=device)\n        self.edgeConv3 = EdgeConv(in_channels=128,out_channels=256,num_neighbours=num_neighbours,device=device)\n\n        self.edgeConv4 = EdgeConv(in_channels=512,out_channels=1024,num_neighbours=num_neighbours,device=device)\n\n        self.linear1 = nn.Linear(2048, 512, bias=False,device=device)\n        self.bn1 = nn.BatchNorm1d(512,device=device)\n        self.drop1 = nn.Dropout(dropout_rate)\n        self.linear2 = nn.Linear(512, 256, bias=False,device=device)\n        self.bn2 = nn.BatchNorm1d(256,device=device)\n        self.drop2 = nn.Dropout(dropout_rate)\n        self.linear3 = nn.Linear(256,out_channels, bias=False,device=device)\n\n\n    def forward(self,x):\n        x0=self.edgeConv0(x)\n        #print(\"x0:\",x0.shape)\n        x1=self.edgeConv1(x0)\n        x2=self.edgeConv2(x1)\n        x3=self.edgeConv3(x2)\n\n        x=torch.cat((x0,x1,x2,x3),dim=2)\n        \n        x= self.edgeConv4(x) # (batch_size, num_points ,64+64+128+256) -> (batch_size, num_points, emb_dims(1024))\n        \n        #todo \n        # maxpool and avgpool\n        x= x.permute(0,2,1).contiguous()\n        maxPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n        avgPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n        x=torch.cat((maxPoolX,avgPoolX),1) #(batch_size, 2048)\n        \n        #mlp[512,256,c(40)]\n        x= F.leaky_relu(self.bn1(self.linear1(x)))\n        x=self.drop1(x)\n        x= F.leaky_relu(self.bn2(self.linear2(x)))\n        x=self.drop2(x)\n        x= self.linear3(x)\n        # output\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-22T04:17:22.729714Z","iopub.execute_input":"2024-04-22T04:17:22.730522Z","iopub.status.idle":"2024-04-22T04:17:22.743810Z","shell.execute_reply.started":"2024-04-22T04:17:22.730493Z","shell.execute_reply":"2024-04-22T04:17:22.742963Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## DGCNN - Classification - Student","metadata":{}},{"cell_type":"code","source":"class DGCNN_S(nn.Module):\n    def __init__(self, num_neighbours=10,out_channels=40,device=\"cpu\"):\n        super(DGCNN_S,self).__init__()\n        self.inChannels=[3,64,64,128,256]\n        self.edgeConv0 = EdgeConv(in_channels=3,out_channels=64,num_neighbours=num_neighbours,device=device)\n        self.edgeConv1 = EdgeConv(in_channels=64,out_channels=64,num_neighbours=num_neighbours,device=device)\n        self.edgeConv2 = EdgeConv(in_channels=64,out_channels=128,num_neighbours=num_neighbours,device=device)\n\n        self.edgeConv4 = EdgeConv(in_channels=256,out_channels=512,num_neighbours=num_neighbours,device=device)\n\n        self.linear1 = nn.Linear(512, 256, bias=False,device=device)\n        self.bn1 = nn.BatchNorm1d(256,device=device)\n        self.linear2 = nn.Linear(256,out_channels, bias=False,device=device)\n\n\n    def forward(self,x):\n        x0=self.edgeConv0(x)\n        #print(\"x0:\",x0.shape)\n        x1=self.edgeConv1(x0)\n        x2=self.edgeConv2(x1)\n\n        x=torch.cat((x0,x1,x2),dim=2)\n        \n        x= self.edgeConv4(x) # (batch_size, num_points ,64+64+128) -> (batch_size, num_points, emb_dims(256))\n        \n        #todo \n        # maxpool and avgpool\n        x= x.permute(0,2,1).contiguous()\n        x = F.adaptive_max_pool1d(x,1).view(x.shape[0],-1)\n        \n        #mlp[512,256,c(40)]\n        x= F.leaky_relu(self.bn1(self.linear1(x)))\n        x= self.linear2(x)\n        # output\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-22T04:17:24.366670Z","iopub.execute_input":"2024-04-22T04:17:24.367483Z","iopub.status.idle":"2024-04-22T04:17:24.377706Z","shell.execute_reply.started":"2024-04-22T04:17:24.367454Z","shell.execute_reply":"2024-04-22T04:17:24.376610Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## DataSet","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport h5py\nimport numpy as np\nfrom torch.utils.data import Dataset\n\ndata_path = \"/kaggle/input/model4/modelnet40_ply_hdf5_2048/modelnet40_ply_hdf5_2048\"\n\ndef load_data_cls(partition):\n    all_data = []\n    all_label = []\n    for h5_name in glob.glob(os.path.join(data_path, '*%s*.h5'%partition)):\n        f = h5py.File(h5_name, 'r')\n        data = f['data'][:].astype('float32')\n        label = f['label'][:].astype('int64')\n        f.close()\n        all_data.append(data)\n        all_label.append(label)\n    all_data = np.concatenate(all_data, axis=0)\n    all_label = np.concatenate(all_label, axis=0)\n    return all_data, all_label\n\ndef translate_pointcloud(pointcloud):\n    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n       \n    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n    return translated_pointcloud\n\nclass ModelNet40(Dataset):\n    def __init__(self, num_points, partition='train'):\n        self.data, self.label = load_data_cls(partition)\n        self.num_points = num_points\n        self.partition = partition        \n\n    def __getitem__(self, item):\n        pointcloud = self.data[item][:self.num_points]\n        label = self.label[item]\n        if self.partition == 'train':\n            pointcloud = translate_pointcloud(pointcloud)\n            np.random.shuffle(pointcloud)\n        return pointcloud, label\n\n    def __len__(self):\n        return self.data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T04:17:26.439733Z","iopub.execute_input":"2024-04-22T04:17:26.440742Z","iopub.status.idle":"2024-04-22T04:17:26.603181Z","shell.execute_reply.started":"2024-04-22T04:17:26.440712Z","shell.execute_reply":"2024-04-22T04:17:26.602362Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Teacher Training","metadata":{}},{"cell_type":"code","source":"traning\nfrom torch.utils.data import DataLoader\nimport sklearn.metrics as metrics\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nnumPoints = 1024\nbatchSize = 16\ntestBatchSize = 8\n_lr = 0.001\nepochs = 50\nPATH = \"./teacher_model.t7\"\n\ntrain_loader = DataLoader(ModelNet40(partition='train',num_points=numPoints), num_workers=2,\n                              batch_size=batchSize, shuffle=True, drop_last=True)\ntest_loader = DataLoader(ModelNet40(partition='test', num_points=numPoints), num_workers=2,\n                             batch_size=testBatchSize, shuffle=True, drop_last=False)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndgcnn = DGCNN(device=device)\n\nopt = optim.Adam(dgcnn.parameters(), lr=_lr, weight_decay=1e-4)\n\nscheduler = CosineAnnealingLR(opt, epochs, eta_min=_lr)\n\nbest_test_acc = 0\n\nfor epoch in range(epochs):\n    ####################\n    # Train\n    ####################\n    train_loss = 0.0\n    count = 0.0\n    dgcnn.train()\n    train_pred = []\n    train_true = []\n    for data, label in train_loader:\n        data, label = data.to(device), label.to(device).squeeze()\n        batch_size = data.size()[0]\n        opt.zero_grad()\n        logits = dgcnn(data)\n        loss = F.cross_entropy(logits, label, reduction='mean')\n        loss.backward()\n        opt.step()\n        scheduler.step()\n        preds = logits.max(dim=1)[1]\n        count += batch_size\n        train_loss += loss.item() * batch_size\n        train_true.append(label.cpu().numpy())\n        train_pred.append(preds.detach().cpu().numpy())\n    train_true = np.concatenate(train_true)\n    train_pred = np.concatenate(train_pred)\n    outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,train_loss*1.0/count,metrics.accuracy_score\n                                                (train_true, train_pred),metrics.balanced_accuracy_score\n                                                (train_true, train_pred))\n    print(outstr)\n    \n    ####################\n    # Test\n    ####################\n    test_loss = 0.0\n    count = 0.0\n    dgcnn.eval()\n    test_pred = []\n    test_true = []\n    for data, label in test_loader:\n        data, label = data.to(device), label.to(device).squeeze()\n        batch_size = data.size()[0]\n        logits = dgcnn(data)\n        loss = F.cross_entropy(logits, label, reduction='mean')\n        preds = logits.max(dim=1)[1]\n        count += batch_size\n        test_loss += loss.item() * batch_size\n        test_true.append(label.cpu().numpy())\n        test_pred.append(preds.detach().cpu().numpy())\n    test_true = np.concatenate(test_true)\n    test_pred = np.concatenate(test_pred)\n    test_acc = metrics.accuracy_score(test_true, test_pred)\n    avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n    outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,test_loss*1.0/count,test_acc,avg_per_class_acc)\n    print(outstr)\n    if test_acc >= best_test_acc:\n        best_test_acc = test_acc\n        torch.save(dgcnn.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:49:11.302556Z","iopub.execute_input":"2024-04-03T15:49:11.302977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Knowledge Distillation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nlr = 0.001\nnum_epochs = 50\nnum_points = 1024\nbatch_size = 16\ntemperature = 2.0\napha = 0.5\nbeta = 0.5\nPATH = \"/kaggle/input/model5/teacher_model.t7\"\n\nclass KnowledgeDistillationLoss(nn.Module):\n    def __init__(self, temperature=1.0):\n        super(KnowledgeDistillationLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, student_logits, teacher_logits):\n        student_probs = F.softmax(student_logits / self.temperature, dim=1)\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        loss = F.kl_div(student_probs.log(), teacher_probs, reduction='batchmean')\n        return loss\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_loader = DataLoader(ModelNet40(num_points, 'train'), batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(ModelNet40(num_points, 'test'), batch_size=batch_size, shuffle=False)\nteacher_model = DGCNN(device=device)\nteacher_model.load_state_dict(torch.load(PATH,map_location=device))\nstudent_model = DGCNN_S(device=device)\noptimizer = optim.Adam(student_model.parameters(), lr)\ndistillation_loss = KnowledgeDistillationLoss(temperature=temperature)\n\nbest_test_acc = 0\nfor epoch in range(num_epochs):\n    #train\n    student_model.train()\n    total_loss = 0\n    for data, label in train_loader:\n        data, label = data.to(device), label.to(device).squeeze()\n        optimizer.zero_grad()\n        student_logits = student_model(data)\n        teacher_logits = teacher_model(data)\n        loss = apha * distillation_loss(student_logits, teacher_logits) + beta * F.cross_entropy(student_logits, label)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {total_loss / len(train_loader):.4f}')\n    #test\n    student_model.eval()\n    total = 0\n    correct = 0\n    total_loss = 0\n    with torch.no_grad():\n        for data, label in test_loader:\n            data, label = data.to(device), label.to(device).squeeze()\n            output = student_model(data)\n            loss = F.cross_entropy(output, label)\n            total_loss += loss.item()\n            _, predicted = output.max(1)\n            total += label.size(0)\n            correct += predicted.eq(label).sum().item()\n    test_acc = correct / total\n    print(f'Epoch {epoch+1}/{num_epochs}, Test Loss: {total_loss / len(test_loader):.4f}, Test Acc: {test_acc:.4f}')\n    if test_acc >= best_test_acc:\n        best_test_acc = test_acc\n        torch.save(student_model.state_dict(), '/kaggle/working/student_model_t2.t7')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:00:31.904172Z","iopub.execute_input":"2024-04-10T08:00:31.904813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport sklearn.metrics as metrics\n\nnum_points = 1024\nbatch_size = 8\n\nTPATH = r\"/kaggle/input/model5/teacher_model.t7\"\nSPATH = r\"/kaggle/input/model5/student_model_t2.t7\"\n\ntest_loader = DataLoader(ModelNet40(num_points, 'test'), batch_size=batch_size, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nteacher_model = DGCNN(device=device)\nstudent_model = DGCNN_S(device=device)\n\nteacher_model.load_state_dict(torch.load(TPATH,map_location=device))\nstudent_model.load_state_dict(torch.load(SPATH,map_location=device))\n\nteacher_model.eval()\nstudent_model.eval()\n\ncount = 0.0\ntest_true = []\n\nt_test_pred = []\ns_test_pred = []\n\nfor data, label in test_loader:\n    data, label = data.to(device), label.to(device).squeeze()\n    soutput = student_model(data)\n    toutput = teacher_model(data)\n    _, spreds = soutput.max(1)\n    _, tpreds = toutput.max(1)\n    count += label.size(0)\n    test_true.append(label.cpu().numpy())\n    t_test_pred.append(tpreds.detach().cpu().numpy())\n    s_test_pred.append(spreds.detach().cpu().numpy())\ntest_true = np.concatenate(test_true)\ns_test_pred = np.concatenate(s_test_pred)\nt_test_pred = np.concatenate(t_test_pred)\ns_test_acc = metrics.accuracy_score(test_true, s_test_pred)\nt_test_acc = metrics.accuracy_score(test_true, t_test_pred)\nt_avg_per_class_acc = metrics.balanced_accuracy_score(test_true, t_test_pred)\ns_avg_per_class_acc = metrics.balanced_accuracy_score(test_true, s_test_pred)\nt_outstr = 'teacher: test acc: %.6f, test avg acc: %.6f' % (t_test_acc,t_avg_per_class_acc)\ns_outstr = 'student: test acc: %.6f, test avg acc: %.6f' % (s_test_acc,s_avg_per_class_acc)\nprint(t_outstr)\nprint(s_outstr)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T04:19:28.559034Z","iopub.execute_input":"2024-04-22T04:19:28.559521Z","iopub.status.idle":"2024-04-22T04:20:13.359218Z","shell.execute_reply.started":"2024-04-22T04:19:28.559487Z","shell.execute_reply":"2024-04-22T04:20:13.358120Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"teacher: test acc: 0.896677, test avg acc: 0.858017\nstudent: test acc: 0.920989, test avg acc: 0.879750\n","output_type":"stream"}]}]}