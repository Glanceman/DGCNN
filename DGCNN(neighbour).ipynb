{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f93d170",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T01:14:53.248879Z",
     "iopub.status.busy": "2024-04-22T01:14:53.248182Z",
     "iopub.status.idle": "2024-04-22T01:14:56.627705Z",
     "shell.execute_reply": "2024-04-22T01:14:56.626949Z"
    },
    "papermill": {
     "duration": 3.389198,
     "end_time": "2024-04-22T01:14:56.629944",
     "exception": false,
     "start_time": "2024-04-22T01:14:53.240746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc1de5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T01:14:56.639415Z",
     "iopub.status.busy": "2024-04-22T01:14:56.639026Z",
     "iopub.status.idle": "2024-04-22T01:14:56.767984Z",
     "shell.execute_reply": "2024-04-22T01:14:56.766921Z"
    },
    "papermill": {
     "duration": 0.135855,
     "end_time": "2024-04-22T01:14:56.770102",
     "exception": false,
     "start_time": "2024-04-22T01:14:56.634247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 100, 20])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "def knn(data, k=5)->torch.Tensor:\n",
    "    \"\"\"Construct edge feature for each point\n",
    "    Args:\n",
    "      point_cloud: (batch_size, num_points, num_dims)\n",
    "      k: int number of neighbours\n",
    "\n",
    "    Returns:\n",
    "      idx: shape:(batch_size, num_points, nums_neighours,)\n",
    "    \"\"\"\n",
    "    dists_matrix = torch.cdist(data, data)\n",
    "    #print(dists_matrix.shape)\n",
    "    _, idx = dists_matrix.topk(k+1, dim=-1, largest=False)  # +1 the point itself is included\n",
    "    return idx[...,1:] # not include the point itself\n",
    "\n",
    "\n",
    "\n",
    "def get_edge_feature(point_cloud, idx=None, k=20,device=\"cpu\"):\n",
    "    \"\"\"Construct edge feature for each point\n",
    "    Args:\n",
    "      point_cloud: (batch_size, num_points, num_dims)\n",
    "      idx: (batch_size, num_points, neighbours)\n",
    "      k: int\n",
    "      device: cpu/cuda\n",
    "\n",
    "    Returns:\n",
    "      features: (batch_size, num_dims ,num_points, k)\n",
    "    \"\"\"\n",
    "    point_cloud = point_cloud.to(device)\n",
    "    batch_size = point_cloud.shape[0]\n",
    "    num_points = point_cloud.shape[1]\n",
    "\n",
    "    if(idx==None):\n",
    "        idx = knn(point_cloud,k=k) # (batch_size, num_points, nums_neighours,)\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1) * num_points # create the base index for mapping\n",
    "    idx = idx.to(device=device)\n",
    "    idx = idx + idx_base #[0...0...0]->[0...100...200]\n",
    "    idx=idx.view(-1) # flatten it -> tensor([  0,  56,  25,  ..., 225, 222, 271], device='cuda:0') e.g: [K01,K02,K03,K11,K12,K13...] shape = (B*N*K) \n",
    "   \n",
    "    num_dims = point_cloud.shape[2]\n",
    "\n",
    "    # feature : turn neighbour index in idx to coordinate\n",
    "    feature = point_cloud.view(batch_size*num_points, -1)[idx, :] # feature : B*N*F -> BN * F -> (B*N*K) * F\n",
    "    # feature : reshape into (Batch_size * Num_points *Nums_neigbours * Features)\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims)\n",
    "    \n",
    "    # pointcloud : create replicate of the self point up to k for matching feature - size B*N*K(repeated)*F \n",
    "    point_cloud = point_cloud.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1) \n",
    "\n",
    "    # feature size B*N*K*F -> B*N*K*2F (feature-x || x)\n",
    "    feature = feature-point_cloud\n",
    "\n",
    "    # (B * 2F * N * K) for later conv each coordinate(F)\n",
    "    feature=feature.permute(0,3,1,2).contiguous()\n",
    "\n",
    "    return feature\n",
    "\n",
    "# Example usage:\n",
    "data = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\n",
    "#neighbors = knn(data, k=4)\n",
    "edges= get_edge_feature(data)\n",
    "print(edges.shape)\n",
    "print(type(edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b2a4d",
   "metadata": {
    "papermill": {
     "duration": 0.003768,
     "end_time": "2024-04-22T01:14:56.777885",
     "exception": false,
     "start_time": "2024-04-22T01:14:56.774117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Edgeconv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeaae6a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T01:14:56.786970Z",
     "iopub.status.busy": "2024-04-22T01:14:56.786677Z",
     "iopub.status.idle": "2024-04-22T01:14:57.766596Z",
     "shell.execute_reply": "2024-04-22T01:14:57.765544Z"
    },
    "papermill": {
     "duration": 0.98699,
     "end_time": "2024-04-22T01:14:57.768724",
     "exception": false,
     "start_time": "2024-04-22T01:14:56.781734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape= torch.Size([3, 100, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EdgeConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_neighbours=20,device=\"cpu\"):\n",
    "        \"\"\"Setup EdgeConv\n",
    "        Args:\n",
    "        in_channels: int\n",
    "        out_channels: int\n",
    "        num_neighbours: int\n",
    "        \"\"\"\n",
    "        super(EdgeConv, self).__init__()\n",
    "        self.device=device\n",
    "        self.k= num_neighbours\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=False,device=self.device),\n",
    "            nn.BatchNorm2d(out_channels,device=self.device),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"Setup EdgeConv\n",
    "        Args:\n",
    "        x: shape - (batch_size, num_points, num_dims)\n",
    "\n",
    "        Returns:\n",
    "        features: (batch_size, num_dims, num_points, num_neigbours)\n",
    "        \"\"\"\n",
    "        x = get_edge_feature(x, k=self.k,device=self.device) #(batch_size, num_points, dim) -> (batch_size, dim*2, num_points ,k)\n",
    "        x = self.conv(x)\n",
    "        # for each point pick the largest k (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "        x = x.max(dim=-1, keepdim=False)[0]\n",
    "        x = x.permute(0,2,1).contiguous()\n",
    "        return x\n",
    "    \n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\n",
    "conv = EdgeConv(3, 64,device=device)\n",
    "out = conv(data)\n",
    "print(\"out.shape=\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84abce",
   "metadata": {
    "papermill": {
     "duration": 0.003821,
     "end_time": "2024-04-22T01:14:57.776657",
     "exception": false,
     "start_time": "2024-04-22T01:14:57.772836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **DGCNN (Classification)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e527c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T01:14:57.786400Z",
     "iopub.status.busy": "2024-04-22T01:14:57.786112Z",
     "iopub.status.idle": "2024-04-22T01:14:57.919004Z",
     "shell.execute_reply": "2024-04-22T01:14:57.917963Z"
    },
    "papermill": {
     "duration": 0.140497,
     "end_time": "2024-04-22T01:14:57.921115",
     "exception": false,
     "start_time": "2024-04-22T01:14:57.780618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  torch.Size([3, 100, 3])\n",
      "cuda\n",
      "out.shape= torch.Size([3, 40])\n"
     ]
    }
   ],
   "source": [
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, num_neighbours=20,out_channels=40,dropout_rate =0.3,device=\"cpu\"):\n",
    "        super(DGCNN,self).__init__()\n",
    "        self.inChannels=[3,64,64,128,256]\n",
    "        self.edgeConv0 = EdgeConv(in_channels=3,out_channels=64,num_neighbours=num_neighbours,device=device)\n",
    "        self.edgeConv1 = EdgeConv(in_channels=64,out_channels=64,num_neighbours=num_neighbours,device=device)\n",
    "        self.edgeConv2 = EdgeConv(in_channels=64,out_channels=128,num_neighbours=num_neighbours,device=device)\n",
    "        self.edgeConv3 = EdgeConv(in_channels=128,out_channels=256,num_neighbours=num_neighbours,device=device)\n",
    "\n",
    "        self.edgeConv4 = EdgeConv(in_channels=512,out_channels=1024,num_neighbours=num_neighbours,device=device)\n",
    "\n",
    "        self.linear1 = nn.Linear(2048, 512, bias=False,device=device)\n",
    "        self.bn1 = nn.BatchNorm1d(512,device=device)\n",
    "        self.drop1 = nn.Dropout(dropout_rate)\n",
    "        self.linear2 = nn.Linear(512, 256, bias=False,device=device)\n",
    "        self.bn2 = nn.BatchNorm1d(256,device=device)\n",
    "        self.drop2 = nn.Dropout(dropout_rate)\n",
    "        self.linear3 = nn.Linear(256,out_channels, bias=False,device=device)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x0=self.edgeConv0(x)\n",
    "        #print(\"x0:\",x0.shape)\n",
    "        x1=self.edgeConv1(x0)\n",
    "        x2=self.edgeConv2(x1)\n",
    "        x3=self.edgeConv3(x2)\n",
    "\n",
    "        x=torch.cat((x0,x1,x2,x3),dim=2)\n",
    "        \n",
    "        x= self.edgeConv4(x) # (batch_size, num_points ,64+64+128+256) -> (batch_size, num_points, emb_dims(1024))\n",
    "        \n",
    "        #todo \n",
    "        # maxpool and avgpool\n",
    "        x= x.permute(0,2,1).contiguous()\n",
    "        maxPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n",
    "        avgPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n",
    "        x=torch.cat((maxPoolX,avgPoolX),1) #(batch_size, 2048)\n",
    "        \n",
    "        #mlp[512,256,c(40)]\n",
    "        x= F.leaky_relu(self.bn1(self.linear1(x)))\n",
    "        x=self.drop1(x)\n",
    "        x= F.leaky_relu(self.bn2(self.linear2(x)))\n",
    "        x=self.drop2(x)\n",
    "        x= self.linear3(x)\n",
    "        # output\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\n",
    "print(\"data shape: \", data.shape)\n",
    "print(device)\n",
    "dgcnn = DGCNN(device=device)\n",
    "out = dgcnn(data)\n",
    "\n",
    "print(\"out.shape=\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064c326",
   "metadata": {
    "papermill": {
     "duration": 0.003857,
     "end_time": "2024-04-22T01:14:57.929123",
     "exception": false,
     "start_time": "2024-04-22T01:14:57.925266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a4bcab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T01:14:57.938136Z",
     "iopub.status.busy": "2024-04-22T01:14:57.937818Z",
     "iopub.status.idle": "2024-04-22T01:15:10.152315Z",
     "shell.execute_reply": "2024-04-22T01:15:10.151247Z"
    },
    "papermill": {
     "duration": 12.221506,
     "end_time": "2024-04-22T01:15:10.154494",
     "exception": false,
     "start_time": "2024-04-22T01:14:57.932988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: cannot verify shapenet.cs.stanford.edu's certificate, issued by 'CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US':\n",
      "  Issued certificate has expired.\n",
      "2024-04-22 01:15:04 URL:https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip [435212151/435212151] -> \"modelnet40_ply_hdf5_2048.zip\" [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  modelnet40_ply_hdf5_2048.zip\n",
      "   creating: modelnet40_ply_hdf5_2048/\n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_2_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train2.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train4.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train1.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/train_files.txt  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_4_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_test1.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_test0.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_test_1_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_1_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_0_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/test_files.txt  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train0.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_test_0_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/shape_names.txt  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train3.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_3_id2file.json  \n",
      "True\n",
      "(1024, 3)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "#todo DataSet -> use mobilenet 40\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def download_modelnet40():\n",
    "    BASE_DIR = os.getcwd()\n",
    "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "    print(os.path.exists(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048')))\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.mkdir(DATA_DIR)\n",
    "    if not os.path.exists(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048')):\n",
    "        www = 'https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip'\n",
    "        zipfile = os.path.basename(www)\n",
    "        os.system('wget --no-verbose --no-check-certificate %s; unzip %s' % (www, zipfile))\n",
    "        os.system('mv %s %s' % ('modelnet40_ply_hdf5_2048', DATA_DIR))\n",
    "        os.system('rm %s' % (zipfile))\n",
    "\n",
    "def load_data_cls(partition):\n",
    "    download_modelnet40()\n",
    "    BASE_DIR = os.getcwd()\n",
    "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "    all_data = []\n",
    "    all_label = []\n",
    "    for h5_name in glob.glob(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048', '*%s*.h5'%partition)):\n",
    "        f = h5py.File(h5_name, 'r+')\n",
    "        data = f['data'][:].astype('float32')\n",
    "        label = f['label'][:].astype('int64')\n",
    "        f.close()\n",
    "        all_data.append(data)\n",
    "        all_label.append(label)\n",
    "    all_data = np.concatenate(all_data, axis=0)\n",
    "    all_label = np.concatenate(all_label, axis=0)\n",
    "    return all_data, all_label\n",
    "\n",
    "def translate_pointcloud(pointcloud):\n",
    "    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n",
    "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
    "       \n",
    "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
    "    return translated_pointcloud\n",
    "\n",
    "class ModelNet40(Dataset):\n",
    "    def __init__(self, num_points, partition='train'):\n",
    "        self.data, self.label = load_data_cls(partition)\n",
    "        self.num_points = num_points\n",
    "        self.partition = partition        \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        pointcloud = self.data[item][:self.num_points]\n",
    "        label = self.label[item]\n",
    "        if self.partition == 'train':\n",
    "            pointcloud = translate_pointcloud(pointcloud)\n",
    "            np.random.shuffle(pointcloud)\n",
    "        return pointcloud, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "# use like this\n",
    "train = ModelNet40(1024)\n",
    "test = ModelNet40(1024, 'test')\n",
    "data, label = train[0]\n",
    "print(data.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6dec0",
   "metadata": {
    "papermill": {
     "duration": 0.004813,
     "end_time": "2024-04-22T01:15:10.164395",
     "exception": false,
     "start_time": "2024-04-22T01:15:10.159582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eae5d93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T01:15:10.175806Z",
     "iopub.status.busy": "2024-04-22T01:15:10.175236Z",
     "iopub.status.idle": "2024-04-22T04:50:30.005494Z",
     "shell.execute_reply": "2024-04-22T04:50:30.004432Z"
    },
    "papermill": {
     "duration": 12919.838233,
     "end_time": "2024-04-22T04:50:30.007524",
     "exception": false,
     "start_time": "2024-04-22T01:15:10.169291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Train 0, loss: 2.965541, train acc: 0.323780, train avg acc: 0.177273\n",
      "Test 0, loss: 2.723888, test acc: 0.357374, test avg acc: 0.239640\n",
      "Train 1, loss: 2.464403, train acc: 0.525508, train avg acc: 0.339394\n",
      "Test 1, loss: 2.365179, test acc: 0.540113, test avg acc: 0.396564\n",
      "Train 2, loss: 2.324223, train acc: 0.576931, train avg acc: 0.401272\n",
      "Test 2, loss: 2.215556, test acc: 0.623582, test avg acc: 0.509297\n",
      "Train 3, loss: 2.247589, train acc: 0.605589, train avg acc: 0.440225\n",
      "Test 3, loss: 2.088943, test acc: 0.671394, test avg acc: 0.562674\n",
      "Train 4, loss: 2.174112, train acc: 0.639939, train avg acc: 0.480076\n",
      "Test 4, loss: 2.039790, test acc: 0.698947, test avg acc: 0.561680\n",
      "Train 5, loss: 2.133051, train acc: 0.662907, train avg acc: 0.507919\n",
      "Test 5, loss: 2.164607, test acc: 0.625203, test avg acc: 0.498535\n",
      "Train 6, loss: 2.116050, train acc: 0.666972, train avg acc: 0.524431\n",
      "Test 6, loss: 1.987226, test acc: 0.725689, test avg acc: 0.625674\n",
      "Train 7, loss: 2.081846, train acc: 0.677134, train avg acc: 0.539012\n",
      "Test 7, loss: 1.988550, test acc: 0.708266, test avg acc: 0.630878\n",
      "Train 8, loss: 2.074691, train acc: 0.682114, train avg acc: 0.544875\n",
      "Test 8, loss: 2.070500, test acc: 0.660454, test avg acc: 0.559703\n",
      "Train 9, loss: 2.049227, train acc: 0.696443, train avg acc: 0.561948\n",
      "Test 9, loss: 1.914625, test acc: 0.743517, test avg acc: 0.648965\n",
      "Train 10, loss: 2.025277, train acc: 0.706402, train avg acc: 0.576532\n",
      "Test 10, loss: 1.926574, test acc: 0.740276, test avg acc: 0.620174\n",
      "Train 11, loss: 2.010231, train acc: 0.710467, train avg acc: 0.580010\n",
      "Test 11, loss: 1.941298, test acc: 0.742301, test avg acc: 0.683727\n",
      "Train 12, loss: 2.002702, train acc: 0.715854, train avg acc: 0.584560\n",
      "Test 12, loss: 1.961460, test acc: 0.760130, test avg acc: 0.690855\n",
      "Train 13, loss: 1.981923, train acc: 0.725102, train avg acc: 0.597578\n",
      "Test 13, loss: 2.020372, test acc: 0.707455, test avg acc: 0.618936\n",
      "Train 14, loss: 1.977138, train acc: 0.728150, train avg acc: 0.604736\n",
      "Test 14, loss: 1.885755, test acc: 0.757293, test avg acc: 0.672651\n",
      "Train 15, loss: 1.972286, train acc: 0.726626, train avg acc: 0.602724\n",
      "Test 15, loss: 1.799235, test acc: 0.805916, test avg acc: 0.679547\n",
      "Train 16, loss: 1.959581, train acc: 0.735976, train avg acc: 0.614708\n",
      "Test 16, loss: 1.838557, test acc: 0.784846, test avg acc: 0.687942\n",
      "Train 17, loss: 1.955180, train acc: 0.733028, train avg acc: 0.613768\n",
      "Test 17, loss: 1.916802, test acc: 0.750810, test avg acc: 0.655994\n",
      "Train 18, loss: 1.950149, train acc: 0.736687, train avg acc: 0.616815\n",
      "Test 18, loss: 1.792152, test acc: 0.825365, test avg acc: 0.732378\n",
      "Train 19, loss: 1.924054, train acc: 0.746850, train avg acc: 0.626187\n",
      "Test 19, loss: 1.804827, test acc: 0.797407, test avg acc: 0.696151\n",
      "Train 20, loss: 1.929299, train acc: 0.743902, train avg acc: 0.629070\n",
      "Test 20, loss: 1.855214, test acc: 0.770259, test avg acc: 0.673640\n",
      "Train 21, loss: 1.921262, train acc: 0.756504, train avg acc: 0.638839\n",
      "Test 21, loss: 1.831943, test acc: 0.786872, test avg acc: 0.726349\n",
      "Train 22, loss: 1.912010, train acc: 0.758232, train avg acc: 0.643508\n",
      "Test 22, loss: 1.868726, test acc: 0.759724, test avg acc: 0.697547\n",
      "Train 23, loss: 1.916238, train acc: 0.752337, train avg acc: 0.642645\n",
      "Test 23, loss: 1.940362, test acc: 0.743112, test avg acc: 0.665517\n",
      "Train 24, loss: 1.903487, train acc: 0.758537, train avg acc: 0.648269\n",
      "Test 24, loss: 1.812728, test acc: 0.786062, test avg acc: 0.685308\n",
      "Train 25, loss: 1.901609, train acc: 0.760772, train avg acc: 0.650332\n",
      "Test 25, loss: 1.819736, test acc: 0.802269, test avg acc: 0.725552\n",
      "Train 26, loss: 1.887366, train acc: 0.763415, train avg acc: 0.650325\n",
      "Test 26, loss: 1.808572, test acc: 0.796596, test avg acc: 0.719541\n",
      "Train 27, loss: 1.886988, train acc: 0.764837, train avg acc: 0.656122\n",
      "Test 27, loss: 1.714104, test acc: 0.836710, test avg acc: 0.742238\n",
      "Train 28, loss: 1.892001, train acc: 0.761890, train avg acc: 0.655027\n",
      "Test 28, loss: 1.801399, test acc: 0.788898, test avg acc: 0.698576\n",
      "Train 29, loss: 1.880528, train acc: 0.765549, train avg acc: 0.658530\n",
      "Test 29, loss: 1.796829, test acc: 0.803079, test avg acc: 0.720279\n",
      "Train 30, loss: 1.879176, train acc: 0.772561, train avg acc: 0.663859\n",
      "Test 30, loss: 1.779335, test acc: 0.811588, test avg acc: 0.714256\n",
      "Train 31, loss: 1.874757, train acc: 0.768699, train avg acc: 0.663568\n",
      "Test 31, loss: 1.742355, test acc: 0.823339, test avg acc: 0.751419\n",
      "Train 32, loss: 1.865980, train acc: 0.770935, train avg acc: 0.665389\n",
      "Test 32, loss: 1.757240, test acc: 0.831442, test avg acc: 0.741948\n",
      "Train 33, loss: 1.871339, train acc: 0.769715, train avg acc: 0.661911\n",
      "Test 33, loss: 1.823285, test acc: 0.793760, test avg acc: 0.720890\n",
      "Train 34, loss: 1.868845, train acc: 0.772663, train avg acc: 0.666449\n",
      "Test 34, loss: 1.756382, test acc: 0.824554, test avg acc: 0.751390\n",
      "Train 35, loss: 1.860142, train acc: 0.775915, train avg acc: 0.670649\n",
      "Test 35, loss: 1.836915, test acc: 0.767423, test avg acc: 0.705919\n",
      "Train 36, loss: 1.862317, train acc: 0.776728, train avg acc: 0.669593\n",
      "Test 36, loss: 1.781938, test acc: 0.790519, test avg acc: 0.710186\n",
      "Train 37, loss: 1.858113, train acc: 0.781809, train avg acc: 0.681264\n",
      "Test 37, loss: 1.788833, test acc: 0.799028, test avg acc: 0.742988\n",
      "Train 38, loss: 1.859674, train acc: 0.779573, train avg acc: 0.678656\n",
      "Test 38, loss: 1.733607, test acc: 0.819287, test avg acc: 0.749523\n",
      "Train 39, loss: 1.854193, train acc: 0.780183, train avg acc: 0.682549\n",
      "Test 39, loss: 1.805631, test acc: 0.789708, test avg acc: 0.734366\n",
      "Train 40, loss: 1.843923, train acc: 0.783943, train avg acc: 0.682240\n",
      "Test 40, loss: 1.983071, test acc: 0.719611, test avg acc: 0.634442\n",
      "Train 41, loss: 1.846348, train acc: 0.787398, train avg acc: 0.690888\n",
      "Test 41, loss: 1.727700, test acc: 0.835494, test avg acc: 0.744512\n",
      "Train 42, loss: 1.852928, train acc: 0.783435, train avg acc: 0.684683\n",
      "Test 42, loss: 1.723165, test acc: 0.823339, test avg acc: 0.750669\n",
      "Train 43, loss: 1.853089, train acc: 0.782012, train avg acc: 0.678948\n",
      "Test 43, loss: 1.768272, test acc: 0.808752, test avg acc: 0.714145\n",
      "Train 44, loss: 1.835464, train acc: 0.790955, train avg acc: 0.692720\n",
      "Test 44, loss: 1.755494, test acc: 0.822528, test avg acc: 0.749349\n",
      "Train 45, loss: 1.843942, train acc: 0.785366, train avg acc: 0.684609\n",
      "Test 45, loss: 1.675723, test acc: 0.861021, test avg acc: 0.777517\n",
      "Train 46, loss: 1.834759, train acc: 0.786890, train avg acc: 0.685781\n",
      "Test 46, loss: 1.817453, test acc: 0.788493, test avg acc: 0.680831\n",
      "Train 47, loss: 1.843786, train acc: 0.784451, train avg acc: 0.681398\n",
      "Test 47, loss: 1.758040, test acc: 0.818071, test avg acc: 0.764413\n",
      "Train 48, loss: 1.838095, train acc: 0.785976, train avg acc: 0.687180\n",
      "Test 48, loss: 1.771713, test acc: 0.817261, test avg acc: 0.762576\n",
      "Train 49, loss: 1.825586, train acc: 0.793191, train avg acc: 0.691591\n",
      "Test 49, loss: 1.798424, test acc: 0.791734, test avg acc: 0.711041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todo , not finish \n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import sklearn.metrics as metrics\n",
    "def cal_loss(pred, gold, smoothing=True):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.2\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "def train(model, epochs, _lr, numPoints,batchSize,device):\n",
    "    \"\"\"train\n",
    "        Args:\n",
    "        model: classifier\n",
    "        dataset: shape(batch_size,nums_point,dimemsion)\n",
    "        optimizerSelect:1 for SGD, 0 for Adam\n",
    "        epochs:train epochs\n",
    "        device:\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(ModelNet40(partition='train', num_points=numPoints), num_workers=4,\n",
    "                              batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(ModelNet40(partition='test', num_points=numPoints), num_workers=4,\n",
    "                             batch_size=batchSize, shuffle=True, drop_last=False)\n",
    "\n",
    "\n",
    "    #Try to load models\n",
    "    model = model\n",
    "    opt = optim.Adam(model.parameters(), lr=_lr, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(opt, epochs, eta_min=_lr)\n",
    "    criterion = cal_loss\n",
    "    best_test_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        ####################\n",
    "        # Train\n",
    "        ####################\n",
    "        train_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        for data, label in train_loader:\n",
    "\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "\n",
    "            batch_size = data.size()[0]\n",
    "            opt.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            train_loss += loss.item() * batch_size\n",
    "            train_true.append(label.cpu().numpy())\n",
    "            train_pred.append(preds.detach().cpu().numpy())\n",
    "\n",
    "        train_true = np.concatenate(train_true)\n",
    "        train_pred = np.concatenate(train_pred)\n",
    "        scheduler.step()\n",
    "        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,train_loss*1.0/count,metrics.accuracy_score\n",
    "                                                    (train_true, train_pred),metrics.balanced_accuracy_score\n",
    "                                                    (train_true, train_pred))\n",
    "    \n",
    "        print(outstr)\n",
    "\n",
    "        ####################\n",
    "        # Test\n",
    "        ####################\n",
    "        test_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            batch_size = data.size()[0]\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, label)\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            test_loss += loss.item() * batch_size\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,test_loss*1.0/count,test_acc,avg_per_class_acc)\n",
    "        print(outstr)\n",
    "        if test_acc >= best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            BASE_DIR = os.getcwd()\n",
    "            DATA_DIR = os.path.join(BASE_DIR, 'checkpoints')\n",
    "            if not os.path.exists(DATA_DIR):\n",
    "                os.mkdir(DATA_DIR)\n",
    "                os.mkdir(os.path.join(DATA_DIR, 'models'))\n",
    "            torch.save(model.state_dict(), 'checkpoints/models/model.t7')    \n",
    "    \n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=DGCNN(device=device).to(device)\n",
    "train(model,50,0.001,1024,8,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966dbcf",
   "metadata": {
    "papermill": {
     "duration": 0.012742,
     "end_time": "2024-04-22T04:50:30.033677",
     "exception": false,
     "start_time": "2024-04-22T04:50:30.020935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f15f0eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T04:50:30.061227Z",
     "iopub.status.busy": "2024-04-22T04:50:30.060554Z",
     "iopub.status.idle": "2024-04-22T04:50:30.065272Z",
     "shell.execute_reply": "2024-04-22T04:50:30.064366Z"
    },
    "papermill": {
     "duration": 0.020667,
     "end_time": "2024-04-22T04:50:30.067057",
     "exception": false,
     "start_time": "2024-04-22T04:50:30.046390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Excute DGCNN\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12941.7107,
   "end_time": "2024-04-22T04:50:32.293933",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-22T01:14:50.583233",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
