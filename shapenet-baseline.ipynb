{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8166695,"sourceType":"datasetVersion","datasetId":4743832},{"sourceId":8182316,"sourceType":"datasetVersion","datasetId":4743220}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:05:39.705167Z","iopub.execute_input":"2024-04-21T08:05:39.705529Z","iopub.status.idle":"2024-04-21T08:05:43.058469Z","shell.execute_reply.started":"2024-04-21T08:05:39.705490Z","shell.execute_reply":"2024-04-21T08:05:43.057511Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def knn(data, k=5)->torch.Tensor:\n    \"\"\"Construct edge feature for each point\n    Args:\n      point_cloud: (batch_size, num_points, num_dims)\n      k: int number of neighbours\n\n    Returns:\n      idx: shape:(batch_size, num_points, nums_neighours,)\n    \"\"\"\n    dists_matrix = torch.cdist(data, data)\n    #print(dists_matrix.shape)\n    _, idx = dists_matrix.topk(k+1, dim=-1, largest=False)  # +1 the point itself is included\n    return idx[...,1:] # not include the point itself\n\n\n\ndef get_edge_feature(point_cloud, idx=None, k=20,device=\"cpu\"):\n    \"\"\"Construct edge feature for each point\n    Args:\n      point_cloud: (batch_size, num_points, num_dims)\n      idx: (batch_size, num_points, neighbours)\n      k: int\n      device: cpu/cuda\n\n    Returns:\n      features: (batch_size, num_dims ,num_points, k)\n    \"\"\"\n    point_cloud = point_cloud.to(device)\n    batch_size = point_cloud.shape[0]\n    num_points = point_cloud.shape[1]\n\n    if(idx==None):\n        idx = knn(point_cloud,k=k) # (batch_size, num_points, nums_neighours,)\n\n    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1) * num_points # create the base index for mapping\n    idx = idx.to(device=device)\n    idx = idx + idx_base #[0...0...0]->[0...100...200]\n    idx=idx.view(-1) # flatten it -> tensor([  0,  56,  25,  ..., 225, 222, 271], device='cuda:0') e.g: [K01,K02,K03,K11,K12,K13...] shape = (B*N*K) \n   \n    num_dims = point_cloud.shape[2]\n\n    # feature : turn neighbour index in idx to coordinate\n    feature = point_cloud.view(batch_size*num_points, -1)[idx, :] # feature : B*N*F -> BN * F -> (B*N*K) * F\n    # feature : reshape into (Batch_size * Num_points *Nums_neigbours * Features)\n    feature = feature.view(batch_size, num_points, k, num_dims)\n    \n    # pointcloud : create replicate of the self point up to k for matching feature - size B*N*K(repeated)*F \n    point_cloud = point_cloud.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1) \n\n    # feature size B*N*K*F -> B*N*K*2F (feature-x || x)\n    feature = torch.cat((feature-point_cloud, point_cloud), dim=3)\n\n    # (B * 2F * N * K) for later conv each coordinate(F)\n    feature=feature.permute(0,3,1,2).contiguous()\n\n    return feature\n\n# Example usage:\ndata = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\n#neighbors = knn(data, k=4)\nedges= get_edge_feature(data)\nprint(edges.shape)\nprint(type(edges))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:05:51.702500Z","iopub.execute_input":"2024-04-21T08:05:51.702939Z","iopub.status.idle":"2024-04-21T08:05:51.721395Z","shell.execute_reply.started":"2024-04-21T08:05:51.702893Z","shell.execute_reply":"2024-04-21T08:05:51.719403Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"torch.Size([3, 6, 100, 20])\n<class 'torch.Tensor'>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Edgeconv**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EdgeConv(nn.Module):\n    def __init__(self, in_channels, out_channels, num_neighbours=20,device=\"cpu\"):\n        \"\"\"Setup EdgeConv\n        Args:\n        in_channels: int\n        out_channels: int\n        num_neighbours: int\n        \"\"\"\n        super(EdgeConv, self).__init__()\n        self.device=device\n        self.k= num_neighbours\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels*2, out_channels=out_channels, kernel_size=1, bias=False,device=self.device),\n            nn.BatchNorm2d(out_channels,device=self.device),\n            nn.LeakyReLU(negative_slope=0.2)\n        )\n\n    def forward(self,x):\n        \"\"\"Setup EdgeConv\n        Args:\n        x: shape - (batch_size, num_points, num_dims)\n\n        Returns:\n        features: (batch_size, num_dims, num_points, num_neigbours)\n        \"\"\"\n        x = get_edge_feature(x, k=self.k,device=self.device) #(batch_size, num_points, dim) -> (batch_size, dim*2, num_points ,k)\n        x = self.conv(x)\n        # for each point pick the largest k (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n        x = x.max(dim=-1, keepdim=False)[0]\n        x = x.permute(0,2,1).contiguous()\n        return x\n    \n# Example usage:\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndata = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\nconv = EdgeConv(3, 64,device=device)\nout = conv(data)\nprint(\"out.shape=\", out.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:05:59.047009Z","iopub.execute_input":"2024-04-21T08:05:59.047904Z","iopub.status.idle":"2024-04-21T08:06:00.017154Z","shell.execute_reply.started":"2024-04-21T08:05:59.047872Z","shell.execute_reply":"2024-04-21T08:06:00.016228Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"out.shape= torch.Size([3, 100, 64])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **DGCNN (Classification)**","metadata":{}},{"cell_type":"code","source":"class DGCNN(nn.Module):\n    def __init__(self, num_neighbours=20,out_channels=40,dropout_rate =0.3,device=\"cpu\"):\n        super(DGCNN,self).__init__()\n        self.inChannels=[3,64,64,128,256]\n        self.edgeConv0 = EdgeConv(in_channels=3,out_channels=64,num_neighbours=num_neighbours,device=device)\n        self.edgeConv1 = EdgeConv(in_channels=64,out_channels=64,num_neighbours=num_neighbours,device=device)\n        self.edgeConv2 = EdgeConv(in_channels=64,out_channels=128,num_neighbours=num_neighbours,device=device)\n        self.edgeConv3 = EdgeConv(in_channels=128,out_channels=256,num_neighbours=num_neighbours,device=device)\n\n        self.edgeConv4 = EdgeConv(in_channels=512,out_channels=1024,num_neighbours=num_neighbours,device=device)\n\n        self.linear1 = nn.Linear(2048, 512, bias=False,device=device)\n        self.bn1 = nn.BatchNorm1d(512,device=device)\n        self.drop1 = nn.Dropout(dropout_rate)\n        self.linear2 = nn.Linear(512, 256, bias=False,device=device)\n        self.bn2 = nn.BatchNorm1d(256,device=device)\n        self.drop2 = nn.Dropout(dropout_rate)\n        self.linear3 = nn.Linear(256,out_channels, bias=False,device=device)\n\n\n    def forward(self,x):\n        x0=self.edgeConv0(x)\n        #print(\"x0:\",x0.shape)\n        x1=self.edgeConv1(x0)\n        x2=self.edgeConv2(x1)\n        x3=self.edgeConv3(x2)\n\n        x=torch.cat((x0,x1,x2,x3),dim=2)\n        \n        x= self.edgeConv4(x) # (batch_size, num_points ,64+64+128+256) -> (batch_size, num_points, emb_dims(1024))\n        \n        #todo \n        # maxpool and avgpool\n        x= x.permute(0,2,1).contiguous()\n        maxPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n        avgPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n        x=torch.cat((maxPoolX,avgPoolX),1) #(batch_size, 2048)\n        \n        #mlp[512,256,c(40)]\n        x= F.leaky_relu(self.bn1(self.linear1(x)))\n        x=self.drop1(x)\n        x= F.leaky_relu(self.bn2(self.linear2(x)))\n        x=self.drop2(x)\n        x= self.linear3(x)\n        # output\n\n        return x\n    \n# Example usage:\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndata = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\nprint(\"data shape: \", data.shape)\nprint(device)\ndgcnn = DGCNN(device=device)\nout = dgcnn(data)\n\nprint(\"out.shape=\", out.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:06:02.006867Z","iopub.execute_input":"2024-04-21T08:06:02.007241Z","iopub.status.idle":"2024-04-21T08:06:02.127626Z","shell.execute_reply.started":"2024-04-21T08:06:02.007211Z","shell.execute_reply":"2024-04-21T08:06:02.126739Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"data shape:  torch.Size([3, 100, 3])\ncuda\nout.shape= torch.Size([3, 40])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset Part","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport h5py\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport json\nimport warnings\ndef pc_normalize(pc):\n    centroid = np.mean(pc, axis=0)\n    pc = pc - centroid\n    m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n    pc = pc / m\n    return pc\n\nclass ShapeNet(Dataset):\n    def __init__(self,root, npoints=1024, split='train', class_choice=None, normal_channel=False):\n        self.npoints = npoints\n        self.root = root\n        self.catfile = os.path.join(self.root, 'synsetoffset2category.txt')\n        self.cat = {}\n        self.normal_channel = normal_channel\n\n\n        with open(self.catfile, 'r') as f:\n            for line in f:\n                ls = line.strip().split()\n                self.cat[ls[0]] = ls[1]\n        self.cat = {k: v for k, v in self.cat.items()}\n        self.classes_original = dict(zip(self.cat, range(len(self.cat))))\n\n        if not class_choice is  None:\n            self.cat = {k:v for k,v in self.cat.items() if k in class_choice}\n        # print(self.cat)\n\n        self.meta = {}\n        with open(os.path.join(self.root, 'train_test_split', 'shuffled_train_file_list.json'), 'r') as f:\n            train_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n        with open(os.path.join(self.root, 'train_test_split', 'shuffled_val_file_list.json'), 'r') as f:\n            val_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n        with open(os.path.join(self.root, 'train_test_split', 'shuffled_test_file_list.json'), 'r') as f:\n            test_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n        for item in self.cat:\n            # print('category', item)\n            self.meta[item] = []\n            dir_point = os.path.join(self.root, self.cat[item])\n            fns = sorted(os.listdir(dir_point))\n            # print(fns[0][0:-4])\n            if split == 'trainval':\n                fns = [fn for fn in fns if ((fn[0:-4] in train_ids) or (fn[0:-4] in val_ids))]\n            elif split == 'train':\n                fns = [fn for fn in fns if fn[0:-4] in train_ids]\n            elif split == 'val':\n                fns = [fn for fn in fns if fn[0:-4] in val_ids]\n            elif split == 'test':\n                fns = [fn for fn in fns if fn[0:-4] in test_ids]\n            else:\n                print('Unknown split: %s. Exiting..' % (split))\n                exit(-1)\n\n            # print(os.path.basename(fns))\n            for fn in fns:\n                token = (os.path.splitext(os.path.basename(fn))[0])\n                self.meta[item].append(os.path.join(dir_point, token + '.txt'))\n\n        self.datapath = []\n        for item in self.cat:\n            for fn in self.meta[item]:\n                self.datapath.append((item, fn))\n\n        self.classes = {}\n        for i in self.cat.keys():\n            self.classes[i] = self.classes_original[i]\n\n        # Mapping from category ('Chair') to a list of int [10,11,12,13] as segmentation labels\n        self.seg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43],\n                            'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46],\n                            'Mug': [36, 37], 'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27],\n                            'Table': [47, 48, 49], 'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40],\n                            'Chair': [12, 13, 14, 15], 'Knife': [22, 23]}\n\n        # for cat in sorted(self.seg_classes.keys()):\n        #     print(cat, self.seg_classes[cat])\n\n        self.cache = {}  # from index to (point_set, cls, seg) tuple\n        self.cache_size = 20000\n\n\n    def __getitem__(self, index):\n        if index in self.cache:\n            ppoint_set, cls, seg = self.cache[index]\n        else:\n            fn = self.datapath[index]\n            cat = self.datapath[index][0]\n            cls = self.classes[cat]\n            cls = np.array([cls]).astype(np.int32)\n            data = np.loadtxt(fn[1]).astype(np.float32)\n            if not self.normal_channel:\n                point_set = data[:, 0:3]\n            else:\n                point_set = data[:, 0:6]\n            seg = data[:, -1].astype(np.int32)\n            if len(self.cache) < self.cache_size:\n                self.cache[index] = (point_set, cls, seg)\n        point_set[:, 0:3] = pc_normalize(point_set[:, 0:3])\n\n        choice = np.random.choice(len(seg), self.npoints, replace=True)\n        # resample\n        point_set = point_set[choice, :]\n        seg = seg[choice]\n\n        return point_set, cls\n\n    def __len__(self):\n        return len(self.datapath)\n_root='/kaggle/input/model4/shapenetcore_partanno_segmentation_benchmark_v0_normal/shapenetcore_partanno_segmentation_benchmark_v0_normal'\ndata = ShapeNet(npoints=1024,root=_root,split='test')\n_data, label= data[0]\nprint(_data.shape)\nprint(label.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:30:16.980422Z","iopub.execute_input":"2024-04-21T08:30:16.980827Z","iopub.status.idle":"2024-04-21T08:30:17.103362Z","shell.execute_reply.started":"2024-04-21T08:30:16.980798Z","shell.execute_reply":"2024-04-21T08:30:17.102327Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(1024, 3)\n(1,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# import os\n# import h5py\n# import torch\n# from torch.utils.data import Dataset\n\n# train_list = ['ply_data_train0.h5', 'ply_data_train1.h5', 'ply_data_train2.h5', 'ply_data_train3.h5', 'ply_data_train4.h5', 'ply_data_train5.h5']\n# test_list = ['ply_data_test0.h5', 'ply_data_test1.h5']\n# val_list = ['ply_data_val0.h5']\n\n# def make_data(mode='train', path='/kaggle/input/model4/shapenet_part_seg_hdf5_data/hdf5_data', num_point=1024):\n#     datas = []\n#     labels = []\n#     if mode == 'train':\n#         for file_list in train_list:\n#             f = h5py.File(os.path.join(path, file_list), 'r')\n#             datas.extend(f['data'][:, :num_point, :])\n#             labels.extend(f['label'])\n#             f.close()\n#     elif mode == 'test':\n#         for file_list in test_list:\n#             f = h5py.File(os.path.join(path, file_list), 'r')\n#             datas.extend(f['data'][:, :num_point, :])\n#             labels.extend(f['label'])\n#             f.close()\n#     else:\n#         for file_list in val_list:\n#             f = h5py.File(os.path.join(path, file_list), 'r')\n#             datas.extend(f['data'][:, :num_point, :])\n#             labels.extend(f['label'])\n#             f.close()\n    \n#     return datas, labels\n\n# class PointDataset(torch.utils.data.Dataset):\n#     def __init__(self, datas, labels):\n#         super().__init__()\n#         self.datas = datas\n#         self.labels = labels\n    \n#     def __getitem__(self, index):\n#         data = torch.tensor(self.datas[index].T.astype('float32'))\n#         label = torch.tensor(self.labels[index].astype('int64'))\n#         data = data.transpose(0, 1)\n#         return data, label\n\n#     def __len__(self):\n#         return len(self.datas)\n\n# datas, labels = make_data(mode='train', num_point=1024)\n# train_dataset = PointDataset(datas, labels)\n# _data, label= train_dataset[0]\n# print(_data.shape)\n# print(label.shape)\n# datas, labels = make_data(mode='val', num_point=1024)\n# val_dataset = PointDataset(datas, labels)\n# _data, label= val_dataset[0]\n# print(_data.shape)\n# print(label.shape)\n# datas, labels = make_data(mode='test', num_point=1024)\n# test_dataset = PointDataset(datas, labels)\n# _data, label= test_dataset[0]\n# print(_data.shape)\n# print(label.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T02:42:37.233415Z","iopub.execute_input":"2024-04-21T02:42:37.233792Z","iopub.status.idle":"2024-04-21T02:42:41.450213Z","shell.execute_reply.started":"2024-04-21T02:42:37.233765Z","shell.execute_reply":"2024-04-21T02:42:41.449206Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([1024, 3])\ntorch.Size([1])\ntorch.Size([1024, 3])\ntorch.Size([1])\ntorch.Size([1024, 3])\ntorch.Size([1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training part","metadata":{}},{"cell_type":"code","source":"#todo , not finish \nfrom torch.utils.data import DataLoader\nimport os\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport sklearn.metrics as metrics\ndef cal_loss(pred, gold, smoothing=True):\n    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n\n    gold = gold.contiguous().view(-1)\n\n    if smoothing:\n        eps = 0.2\n        n_class = pred.size(1)\n\n        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(pred, dim=1)\n\n        loss = -(one_hot * log_prb).sum(dim=1).mean()\n    else:\n        loss = F.cross_entropy(pred, gold, reduction='mean')\n\n    return loss\ndef train(model, epochs, _lr, numPoints,batchSize,device):\n    \"\"\"train\n        Args:\n        model: classifier\n        dataset: shape(batch_size,nums_point,dimemsion)\n        optimizerSelect:1 for SGD, 0 for Adam\n        epochs:train epochs\n        device:\n    \"\"\"\n\n    train_loader = DataLoader(ShapeNet(npoints=1024,root=_root,split='train'),\n                              batch_size=batchSize, shuffle=True)\n\n    test_loader = DataLoader(ShapeNet(npoints=1024,root=_root,split='test'), \n                             batch_size=batchSize, shuffle=True)\n\n\n    #Try to load models\n    model = nn.DataParallel(model)\n    opt = optim.Adam(model.parameters(), lr=_lr, weight_decay=1e-4)\n    scheduler = CosineAnnealingLR(opt, epochs, eta_min=_lr)\n    criterion = cal_loss\n    best_test_acc = 0\n    i=0\n    for epoch in range(epochs):\n        print(\"test1\")\n        scheduler.step()\n        \n        ####################\n        # Train\n        ####################\n        train_loss = 0.0\n        count = 0.0\n        model.train()\n        train_pred = []\n        train_true = []\n        for data, label in train_loader:\n            data, label = data.to(device), label.to(device).squeeze()\n            batch_size = data.size()[0]\n            opt.zero_grad()\n            logits = model(data)\n            loss = criterion(logits, label)\n            loss.backward()\n            opt.step()\n            preds = logits.max(dim=1)[1]\n            count += batch_size\n            train_loss += loss.item() * batch_size\n            train_true.append(label.cpu().numpy())\n            train_pred.append(preds.detach().cpu().numpy())\n            print(\"test2\")\n        print(\"test3\")\n        train_true = np.concatenate(train_true)\n        train_pred = np.concatenate(train_pred)\n        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,train_loss*1.0/count,metrics.accuracy_score\n                                                    (train_true, train_pred),metrics.balanced_accuracy_score\n                                                    (train_true, train_pred))\n    \n        print(outstr)\n\n        ####################\n        # Test\n        ####################\n        test_loss = 0.0\n        count = 0.0\n        model.eval()\n        test_pred = []\n        test_true = []\n        for data, label in test_loader:\n            data, label = data.to(device), label.to(device).squeeze()\n            batch_size = data.size()[0]\n            logits = model(data)\n            loss = criterion(logits, label)\n            preds = logits.max(dim=1)[1]\n            count += batch_size\n            test_loss += loss.item() * batch_size\n            test_true.append(label.cpu().numpy())\n            test_pred.append(preds.detach().cpu().numpy())\n\n        test_true = np.concatenate(test_true)\n        test_pred = np.concatenate(test_pred)\n        test_acc = metrics.accuracy_score(test_true, test_pred)\n        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,test_loss*1.0/count,test_acc,avg_per_class_acc)\n        print(outstr)\n        if test_acc >= best_test_acc:\n            best_test_acc = test_acc\n            BASE_DIR = os.getcwd()\n            DATA_DIR = os.path.join(BASE_DIR, 'checkpoints')\n            if not os.path.exists(DATA_DIR):\n                os.mkdir(DATA_DIR)\n                os.mkdir(os.path.join(DATA_DIR, 'models'))\n            torch.save(model.state_dict(), '/kaggle/working/model_shape_5.t7')    \n    \n    \n    return 0\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel=DGCNN(device=device).to(device)\ntrain(model,5,0.001,1024,8,device)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:30:22.615682Z","iopub.execute_input":"2024-04-21T08:30:22.616291Z","iopub.status.idle":"2024-04-21T08:30:25.604500Z","shell.execute_reply.started":"2024-04-21T08:30:22.616261Z","shell.execute_reply":"2024-04-21T08:30:25.603060Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"test1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m model\u001b[38;5;241m=\u001b[39mDGCNN(device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 125\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 66\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, _lr, numPoints, batchSize, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     65\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m---> 66\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     68\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n","Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mcal_loss\u001b[0;34m(pred, gold, smoothing)\u001b[0m\n\u001b[1;32m     13\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m     14\u001b[0m n_class \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m one_hot \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m one_hot) \u001b[38;5;241m*\u001b[39m eps \u001b[38;5;241m/\u001b[39m (n_class \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m log_prb \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: scatter(): Expected dtype int64 for index"],"ename":"RuntimeError","evalue":"scatter(): Expected dtype int64 for index","output_type":"error"}]},{"cell_type":"markdown","source":"## Run Code","metadata":{}},{"cell_type":"code","source":"## Excute DGCNN\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T02:45:31.075565Z","iopub.status.idle":"2024-04-21T02:45:31.075986Z","shell.execute_reply.started":"2024-04-21T02:45:31.075783Z","shell.execute_reply":"2024-04-21T02:45:31.075800Z"},"trusted":true},"execution_count":null,"outputs":[]}]}