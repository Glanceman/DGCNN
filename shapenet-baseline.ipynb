{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T01:32:27.511757Z","iopub.status.busy":"2024-04-21T01:32:27.511352Z","iopub.status.idle":"2024-04-21T01:32:27.516334Z","shell.execute_reply":"2024-04-21T01:32:27.515420Z","shell.execute_reply.started":"2024-04-21T01:32:27.511727Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T01:32:26.004687Z","iopub.status.busy":"2024-04-21T01:32:26.004072Z","iopub.status.idle":"2024-04-21T01:32:26.139158Z","shell.execute_reply":"2024-04-21T01:32:26.138153Z","shell.execute_reply.started":"2024-04-21T01:32:26.004658Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 6, 100, 20])\n","<class 'torch.Tensor'>\n"]}],"source":["def knn(data, k=5)->torch.Tensor:\n","    \"\"\"Construct edge feature for each point\n","    Args:\n","      point_cloud: (batch_size, num_points, num_dims)\n","      k: int number of neighbours\n","\n","    Returns:\n","      idx: shape:(batch_size, num_points, nums_neighours,)\n","    \"\"\"\n","    dists_matrix = torch.cdist(data, data)\n","    #print(dists_matrix.shape)\n","    _, idx = dists_matrix.topk(k+1, dim=-1, largest=False)  # +1 the point itself is included\n","    return idx[...,1:] # not include the point itself\n","\n","\n","\n","def get_edge_feature(point_cloud, idx=None, k=20,device=\"cpu\"):\n","    \"\"\"Construct edge feature for each point\n","    Args:\n","      point_cloud: (batch_size, num_points, num_dims)\n","      idx: (batch_size, num_points, neighbours)\n","      k: int\n","      device: cpu/cuda\n","\n","    Returns:\n","      features: (batch_size, num_dims ,num_points, k)\n","    \"\"\"\n","    point_cloud = point_cloud.to(device)\n","    batch_size = point_cloud.shape[0]\n","    num_points = point_cloud.shape[1]\n","\n","    if(idx==None):\n","        idx = knn(point_cloud,k=k) # (batch_size, num_points, nums_neighours,)\n","\n","    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1) * num_points # create the base index for mapping\n","    idx = idx.to(device=device)\n","    idx = idx + idx_base #[0...0...0]->[0...100...200]\n","    idx=idx.view(-1) # flatten it -> tensor([  0,  56,  25,  ..., 225, 222, 271], device='cuda:0') e.g: [K01,K02,K03,K11,K12,K13...] shape = (B*N*K) \n","   \n","    num_dims = point_cloud.shape[2]\n","\n","    # feature : turn neighbour index in idx to coordinate\n","    feature = point_cloud.view(batch_size*num_points, -1)[idx, :] # feature : B*N*F -> BN * F -> (B*N*K) * F\n","    # feature : reshape into (Batch_size * Num_points *Nums_neigbours * Features)\n","    feature = feature.view(batch_size, num_points, k, num_dims)\n","    \n","    # pointcloud : create replicate of the self point up to k for matching feature - size B*N*K(repeated)*F \n","    point_cloud = point_cloud.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1) \n","\n","    # feature size B*N*K*F -> B*N*K*2F (feature-x || x)\n","    feature = torch.cat((feature-point_cloud, point_cloud), dim=3)\n","\n","    # (B * 2F * N * K) for later conv each coordinate(F)\n","    feature=feature.permute(0,3,1,2).contiguous()\n","\n","    return feature\n","\n","# Example usage:\n","data = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\n","#neighbors = knn(data, k=4)\n","edges= get_edge_feature(data)\n","print(edges.shape)\n","print(type(edges))"]},{"cell_type":"markdown","metadata":{},"source":["### **Edgeconv**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T01:32:29.814238Z","iopub.status.busy":"2024-04-21T01:32:29.813204Z","iopub.status.idle":"2024-04-21T01:32:29.888875Z","shell.execute_reply":"2024-04-21T01:32:29.887930Z","shell.execute_reply.started":"2024-04-21T01:32:29.814203Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["out.shape= torch.Size([3, 100, 64])\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class EdgeConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, num_neighbours=20,device=\"cpu\"):\n","        \"\"\"Setup EdgeConv\n","        Args:\n","        in_channels: int\n","        out_channels: int\n","        num_neighbours: int\n","        \"\"\"\n","        super(EdgeConv, self).__init__()\n","        self.device=device\n","        self.k= num_neighbours\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels*2, out_channels=out_channels, kernel_size=1, bias=False,device=self.device),\n","            nn.BatchNorm2d(out_channels,device=self.device),\n","            nn.LeakyReLU(negative_slope=0.2)\n","        )\n","\n","    def forward(self,x):\n","        \"\"\"Setup EdgeConv\n","        Args:\n","        x: shape - (batch_size, num_points, num_dims)\n","\n","        Returns:\n","        features: (batch_size, num_dims, num_points, num_neigbours)\n","        \"\"\"\n","        x = get_edge_feature(x, k=self.k,device=self.device) #(batch_size, num_points, dim) -> (batch_size, dim*2, num_points ,k)\n","        x = self.conv(x)\n","        # for each point pick the largest k (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n","        x = x.max(dim=-1, keepdim=False)[0]\n","        x = x.permute(0,2,1).contiguous()\n","        return x\n","    \n","# Example usage:\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","data = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\n","conv = EdgeConv(3, 64,device=device)\n","out = conv(data)\n","print(\"out.shape=\", out.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### **DGCNN (Classification)**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T01:32:32.365052Z","iopub.status.busy":"2024-04-21T01:32:32.364398Z","iopub.status.idle":"2024-04-21T01:32:32.729510Z","shell.execute_reply":"2024-04-21T01:32:32.728489Z","shell.execute_reply.started":"2024-04-21T01:32:32.365022Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["data shape:  torch.Size([3, 100, 3])\n","cpu\n","out.shape= torch.Size([3, 40])\n"]}],"source":["class DGCNN(nn.Module):\n","    def __init__(self, num_neighbours=20,out_channels=40,dropout_rate =0.3,device=\"cpu\"):\n","        super(DGCNN,self).__init__()\n","        self.inChannels=[3,64,64,128,256]\n","        self.edgeConv0 = EdgeConv(in_channels=3,out_channels=64,num_neighbours=num_neighbours,device=device)\n","        self.edgeConv1 = EdgeConv(in_channels=64,out_channels=64,num_neighbours=num_neighbours,device=device)\n","        self.edgeConv2 = EdgeConv(in_channels=64,out_channels=128,num_neighbours=num_neighbours,device=device)\n","        self.edgeConv3 = EdgeConv(in_channels=128,out_channels=256,num_neighbours=num_neighbours,device=device)\n","\n","        self.edgeConv4 = EdgeConv(in_channels=512,out_channels=1024,num_neighbours=num_neighbours,device=device)\n","\n","        self.linear1 = nn.Linear(2048, 512, bias=False,device=device)\n","        self.bn1 = nn.BatchNorm1d(512,device=device)\n","        self.drop1 = nn.Dropout(dropout_rate)\n","        self.linear2 = nn.Linear(512, 256, bias=False,device=device)\n","        self.bn2 = nn.BatchNorm1d(256,device=device)\n","        self.drop2 = nn.Dropout(dropout_rate)\n","        self.linear3 = nn.Linear(256,out_channels, bias=False,device=device)\n","\n","\n","    def forward(self,x):\n","        x0=self.edgeConv0(x)\n","        #print(\"x0:\",x0.shape)\n","        x1=self.edgeConv1(x0)\n","        x2=self.edgeConv2(x1)\n","        x3=self.edgeConv3(x2)\n","\n","        x=torch.cat((x0,x1,x2,x3),dim=2)\n","        \n","        x= self.edgeConv4(x) # (batch_size, num_points ,64+64+128+256) -> (batch_size, num_points, emb_dims(1024))\n","        \n","        #todo \n","        # maxpool and avgpool\n","        x= x.permute(0,2,1).contiguous()\n","        maxPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n","        avgPoolX = F.adaptive_avg_pool1d(x,1).view(x.shape[0],-1)\n","        x=torch.cat((maxPoolX,avgPoolX),1) #(batch_size, 2048)\n","        \n","        #mlp[512,256,c(40)]\n","        x= F.leaky_relu(self.bn1(self.linear1(x)))\n","        x=self.drop1(x)\n","        x= F.leaky_relu(self.bn2(self.linear2(x)))\n","        x=self.drop2(x)\n","        x= self.linear3(x)\n","        # output\n","\n","        return x\n","    \n","# Example usage:\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","data = torch.rand((3,100, 3))  # 100 points in 20D (batch_size, num_points, num_dims)\n","print(\"data shape: \", data.shape)\n","print(device)\n","dgcnn = DGCNN(device=device)\n","out = dgcnn(data)\n","\n","print(\"out.shape=\", out.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Part"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T09:58:00.418595Z","iopub.status.busy":"2024-04-20T09:58:00.418326Z","iopub.status.idle":"2024-04-20T09:58:01.746640Z","shell.execute_reply":"2024-04-20T09:58:01.745163Z","shell.execute_reply.started":"2024-04-20T09:58:00.418572Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(1024, 3)\n","(1,)\n"]}],"source":["import os\n","import glob\n","import h5py\n","import numpy as np\n","from torch.utils.data import Dataset\n","import json\n","import warnings\n","def pc_normalize(pc):\n","    centroid = np.mean(pc, axis=0)\n","    pc = pc - centroid\n","    m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n","    pc = pc / m\n","    return pc\n","\n","class ShapeNet(Dataset):\n","    def __init__(self,root, npoints=1024, split='train', class_choice=None, normal_channel=False):\n","        self.npoints = npoints\n","        self.root = root\n","        self.catfile = os.path.join(self.root, 'synsetoffset2category.txt')\n","        self.cat = {}\n","        self.normal_channel = normal_channel\n","\n","\n","        with open(self.catfile, 'r') as f:\n","            for line in f:\n","                ls = line.strip().split()\n","                self.cat[ls[0]] = ls[1]\n","        self.cat = {k: v for k, v in self.cat.items()}\n","        self.classes_original = dict(zip(self.cat, range(len(self.cat))))\n","\n","        if not class_choice is  None:\n","            self.cat = {k:v for k,v in self.cat.items() if k in class_choice}\n","        # print(self.cat)\n","\n","        self.meta = {}\n","        with open(os.path.join(self.root, 'train_test_split', 'shuffled_train_file_list.json'), 'r') as f:\n","            train_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n","        with open(os.path.join(self.root, 'train_test_split', 'shuffled_val_file_list.json'), 'r') as f:\n","            val_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n","        with open(os.path.join(self.root, 'train_test_split', 'shuffled_test_file_list.json'), 'r') as f:\n","            test_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n","        for item in self.cat:\n","            # print('category', item)\n","            self.meta[item] = []\n","            dir_point = os.path.join(self.root, self.cat[item])\n","            fns = sorted(os.listdir(dir_point))\n","            # print(fns[0][0:-4])\n","            if split == 'trainval':\n","                fns = [fn for fn in fns if ((fn[0:-4] in train_ids) or (fn[0:-4] in val_ids))]\n","            elif split == 'train':\n","                fns = [fn for fn in fns if fn[0:-4] in train_ids]\n","            elif split == 'val':\n","                fns = [fn for fn in fns if fn[0:-4] in val_ids]\n","            elif split == 'test':\n","                fns = [fn for fn in fns if fn[0:-4] in test_ids]\n","            else:\n","                print('Unknown split: %s. Exiting..' % (split))\n","                exit(-1)\n","\n","            # print(os.path.basename(fns))\n","            for fn in fns:\n","                token = (os.path.splitext(os.path.basename(fn))[0])\n","                self.meta[item].append(os.path.join(dir_point, token + '.txt'))\n","\n","        self.datapath = []\n","        for item in self.cat:\n","            for fn in self.meta[item]:\n","                self.datapath.append((item, fn))\n","\n","        self.classes = {}\n","        for i in self.cat.keys():\n","            self.classes[i] = self.classes_original[i]\n","\n","        # Mapping from category ('Chair') to a list of int [10,11,12,13] as segmentation labels\n","        self.seg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43],\n","                            'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46],\n","                            'Mug': [36, 37], 'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27],\n","                            'Table': [47, 48, 49], 'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40],\n","                            'Chair': [12, 13, 14, 15], 'Knife': [22, 23]}\n","\n","        # for cat in sorted(self.seg_classes.keys()):\n","        #     print(cat, self.seg_classes[cat])\n","\n","        self.cache = {}  # from index to (point_set, cls, seg) tuple\n","        self.cache_size = 20000\n","\n","\n","    def __getitem__(self, index):\n","        if index in self.cache:\n","            ppoint_set, cls, seg = self.cache[index]\n","        else:\n","            fn = self.datapath[index]\n","            cat = self.datapath[index][0]\n","            cls = self.classes[cat]\n","            cls = np.array([cls]).astype(np.int32)\n","            data = np.loadtxt(fn[1]).astype(np.float32)\n","            if not self.normal_channel:\n","                point_set = data[:, 0:3]\n","            else:\n","                point_set = data[:, 0:6]\n","            seg = data[:, -1].astype(np.int32)\n","            if len(self.cache) < self.cache_size:\n","                self.cache[index] = (point_set, cls, seg)\n","        point_set[:, 0:3] = pc_normalize(point_set[:, 0:3])\n","\n","        choice = np.random.choice(len(seg), self.npoints, replace=True)\n","        # resample\n","        point_set = point_set[choice, :]\n","        seg = seg[choice]\n","\n","        return point_set, cls\n","\n","    def __len__(self):\n","        return len(self.datapath)\n","_root='/kaggle/input/model4/shapenetcore_partanno_segmentation_benchmark_v0_normal/shapenetcore_partanno_segmentation_benchmark_v0_normal'\n","data = ShapeNet(npoints=1024,root=_root,split='test')\n","_data, label= data[0]\n","print(_data.shape)\n","print(label.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T01:32:37.636396Z","iopub.status.busy":"2024-04-21T01:32:37.635488Z","iopub.status.idle":"2024-04-21T01:32:41.192927Z","shell.execute_reply":"2024-04-21T01:32:41.191777Z","shell.execute_reply.started":"2024-04-21T01:32:37.636361Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 1024])\n","torch.Size([1])\n","torch.Size([3, 1024])\n","torch.Size([1])\n","torch.Size([3, 1024])\n","torch.Size([1])\n"]}],"source":["import os\n","import h5py\n","import torch\n","from torch.utils.data import Dataset\n","\n","train_list = ['ply_data_train0.h5', 'ply_data_train1.h5', 'ply_data_train2.h5', 'ply_data_train3.h5', 'ply_data_train4.h5', 'ply_data_train5.h5']\n","test_list = ['ply_data_test0.h5', 'ply_data_test1.h5']\n","val_list = ['ply_data_val0.h5']\n","\n","def make_data(mode='train', path='/kaggle/input/model4/shapenet_part_seg_hdf5_data/hdf5_data', num_point=1024):\n","    datas = []\n","    labels = []\n","    if mode == 'train':\n","        for file_list in train_list:\n","            f = h5py.File(os.path.join(path, file_list), 'r')\n","            datas.extend(f['data'][:, :num_point, :])\n","            labels.extend(f['label'])\n","            f.close()\n","    elif mode == 'test':\n","        for file_list in test_list:\n","            f = h5py.File(os.path.join(path, file_list), 'r')\n","            datas.extend(f['data'][:, :num_point, :])\n","            labels.extend(f['label'])\n","            f.close()\n","    else:\n","        for file_list in val_list:\n","            f = h5py.File(os.path.join(path, file_list), 'r')\n","            datas.extend(f['data'][:, :num_point, :])\n","            labels.extend(f['label'])\n","            f.close()\n","    \n","    return datas, labels\n","\n","class PointDataset(torch.utils.data.Dataset):\n","    def __init__(self, datas, labels):\n","        super().__init__()\n","        self.datas = datas\n","        self.labels = labels\n","    \n","    def __getitem__(self, index):\n","        data = torch.tensor(self.datas[index].T.astype('float32'))\n","        label = torch.tensor(self.labels[index].astype('int64'))\n","        return data, label\n","\n","    def __len__(self):\n","        return len(self.datas)\n","\n","datas, labels = make_data(mode='train', num_point=1024)\n","train_dataset = PointDataset(datas, labels)\n","_data, label= train_dataset[0]\n","print(_data.shape)\n","print(label.shape)\n","datas, labels = make_data(mode='val', num_point=1024)\n","val_dataset = PointDataset(datas, labels)\n","_data, label= val_dataset[0]\n","print(_data.shape)\n","print(label.shape)\n","datas, labels = make_data(mode='test', num_point=1024)\n","test_dataset = PointDataset(datas, labels)\n","_data, label= test_dataset[0]\n","print(_data.shape)\n","print(label.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Training part"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T01:46:51.741557Z","iopub.status.busy":"2024-04-21T01:46:51.740583Z","iopub.status.idle":"2024-04-21T01:46:56.875294Z","shell.execute_reply":"2024-04-21T01:46:56.873885Z","shell.execute_reply.started":"2024-04-21T01:46:51.741523Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["test\n"]},{"ename":"RuntimeError","evalue":"selected index k out of range","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 122\u001b[0m\n\u001b[1;32m    120\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m model\u001b[38;5;241m=\u001b[39mDGCNN(device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 122\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, _lr, numPoints, batchSize, device)\u001b[0m\n\u001b[1;32m     61\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     62\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 63\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, label)\n\u001b[1;32m     65\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mDGCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 22\u001b[0m     x0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medgeConv0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#print(\"x0:\",x0.shape)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     x1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medgeConv1(x0)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mEdgeConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Setup EdgeConv\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    x: shape - (batch_size, num_points, num_dims)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    features: (batch_size, num_dims, num_points, num_neigbours)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mget_edge_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#(batch_size, num_points, dim) -> (batch_size, dim*2, num_points ,k)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# for each point pick the largest k (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mget_edge_feature\u001b[0;34m(point_cloud, idx, k, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m num_points \u001b[38;5;241m=\u001b[39m point_cloud\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(idx\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 33\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint_cloud\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch_size, num_points, nums_neighours,)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m idx_base \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, batch_size, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m num_points \u001b[38;5;66;03m# create the base index for mapping\u001b[39;00m\n\u001b[1;32m     36\u001b[0m idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n","Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mknn\u001b[0;34m(data, k)\u001b[0m\n\u001b[1;32m     10\u001b[0m dists_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcdist(data, data)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#print(dists_matrix.shape)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m _, idx \u001b[38;5;241m=\u001b[39m \u001b[43mdists_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlargest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# +1 the point itself is included\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idx[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m1\u001b[39m:]\n","\u001b[0;31mRuntimeError\u001b[0m: selected index k out of range"]}],"source":["#todo , not finish \n","from torch.utils.data import DataLoader\n","import os\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","import sklearn.metrics as metrics\n","def cal_loss(pred, gold, smoothing=True):\n","    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n","\n","    gold = gold.contiguous().view(-1)\n","\n","    if smoothing:\n","        eps = 0.2\n","        n_class = pred.size(1)\n","\n","        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n","        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n","        log_prb = F.log_softmax(pred, dim=1)\n","\n","        loss = -(one_hot * log_prb).sum(dim=1).mean()\n","    else:\n","        loss = F.cross_entropy(pred, gold, reduction='mean')\n","\n","    return loss\n","def train(model, epochs, _lr, numPoints,batchSize,device):\n","    \"\"\"train\n","        Args:\n","        model: classifier\n","        dataset: shape(batch_size,nums_point,dimemsion)\n","        optimizerSelect:1 for SGD, 0 for Adam\n","        epochs:train epochs\n","        device:\n","    \"\"\"\n","    datas, labels = make_data(mode='train', num_point=1024)\n","    train_loader = DataLoader(PointDataset(datas, labels),\n","                              batch_size=batchSize, shuffle=True)\n","    datas, labels = make_data(mode='test', num_point=1024)\n","    test_loader = DataLoader(PointDataset(datas, labels), \n","                             batch_size=batchSize, shuffle=True)\n","\n","\n","    #Try to load models\n","    model = nn.DataParallel(model)\n","    opt = optim.Adam(model.parameters(), lr=_lr, weight_decay=1e-4)\n","    scheduler = CosineAnnealingLR(opt, epochs, eta_min=_lr)\n","    criterion = cal_loss\n","    best_test_acc = 0\n","    for epoch in range(epochs):\n","        scheduler.step()\n","        ####################\n","        # Train\n","        ####################\n","        train_loss = 0.0\n","        count = 0.0\n","        model.train()\n","        train_pred = []\n","        train_true = []\n","        for data, label in train_loader:\n","            print('test')\n","            data, label = data.to(device), label.to(device).squeeze()\n","            batch_size = data.size()[0]\n","            opt.zero_grad()\n","            logits = model(data)\n","            loss = criterion(logits, label)\n","            loss.backward()\n","            opt.step()\n","            preds = logits.max(dim=1)[1]\n","            count += batch_size\n","            train_loss += loss.item() * batch_size\n","            train_true.append(label.cpu().numpy())\n","            train_pred.append(preds.detach().cpu().numpy())\n","\n","        train_true = np.concatenate(train_true)\n","        train_pred = np.concatenate(train_pred)\n","        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,train_loss*1.0/count,metrics.accuracy_score\n","                                                    (train_true, train_pred),metrics.balanced_accuracy_score\n","                                                    (train_true, train_pred))\n","    \n","        print(outstr)\n","\n","        ####################\n","        # Test\n","        ####################\n","        test_loss = 0.0\n","        count = 0.0\n","        model.eval()\n","        test_pred = []\n","        test_true = []\n","        for data, label in test_loader:\n","            data, label = data.to(device), label.to(device).squeeze()\n","            batch_size = data.size()[0]\n","            logits = model(data)\n","            loss = criterion(logits, label)\n","            preds = logits.max(dim=1)[1]\n","            count += batch_size\n","            test_loss += loss.item() * batch_size\n","            test_true.append(label.cpu().numpy())\n","            test_pred.append(preds.detach().cpu().numpy())\n","\n","        test_true = np.concatenate(test_true)\n","        test_pred = np.concatenate(test_pred)\n","        test_acc = metrics.accuracy_score(test_true, test_pred)\n","        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n","        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,test_loss*1.0/count,test_acc,avg_per_class_acc)\n","        print(outstr)\n","        if test_acc >= best_test_acc:\n","            best_test_acc = test_acc\n","            BASE_DIR = os.getcwd()\n","            DATA_DIR = os.path.join(BASE_DIR, 'checkpoints')\n","            if not os.path.exists(DATA_DIR):\n","                os.mkdir(DATA_DIR)\n","                os.mkdir(os.path.join(DATA_DIR, 'models'))\n","            torch.save(model.state_dict(), '/kaggle/working/model_shape_5.t7')    \n","    \n","    \n","    return 0\n","\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model=DGCNN(device=device).to(device)\n","train(model,5,0.001,1024,8,device)"]},{"cell_type":"markdown","metadata":{},"source":["## Run Code"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-20T09:59:30.334515Z","iopub.status.idle":"2024-04-20T09:59:30.334885Z","shell.execute_reply":"2024-04-20T09:59:30.334717Z","shell.execute_reply.started":"2024-04-20T09:59:30.334702Z"},"trusted":true},"outputs":[],"source":["## Excute DGCNN\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4743832,"sourceId":8166695,"sourceType":"datasetVersion"},{"datasetId":4743220,"sourceId":8180140,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
